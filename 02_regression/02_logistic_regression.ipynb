{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36aa54b-2f2a-4c85-9d79-c7ac5a6ffd76",
   "metadata": {},
   "source": [
    "# Logistic Regression Primer\n",
    "\n",
    "This is a practical primer for logistic regression implementation\n",
    "\n",
    "**Sections:**\n",
    "- Section 1: Core Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6c51e-d584-405c-9cad-613b53409e71",
   "metadata": {},
   "source": [
    "## 1. Core Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead7f07f-266a-494e-aed8-c45774e02511",
   "metadata": {},
   "source": [
    "### 1.1 Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e491da-15dc-4641-9603-f0ba209b36d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "Features: 30\n",
      "Samples: 569\n",
      "Target classes: ['malignant' 'benign']\n",
      "Class distribution: [212 357]\n",
      "Class balance: 62.7% malignant\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "\n",
    "# Load a binary classification dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(\"Dataset info:\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]}\")\n",
    "print(f\"Target classes: {data.target_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Class balance: {np.bincount(y)[1]/len(y):.1%} malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a03c4c2-faf0-4ebd-b5bd-8079355d2345",
   "metadata": {},
   "source": [
    "### 1.2 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074d3169-670e-4af9-a444-934e5fccdea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split:\n",
      "Training samples: 455\n",
      "Test samples: 114\n",
      "\n",
      "Model trained successfully!\n",
      "Model converged in 2227 iterations\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# stratify ensures that the training and testing sets have the same proportion of classes as the original dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "model = LogisticRegression(random_state=42, max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nModel trained successfully!\")\n",
    "print(f\"Model converged in {model.n_iter_[0]} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4bbb4d-b7f1-4181-a8bb-6efb9a1f497e",
   "metadata": {},
   "source": [
    "### 1.3 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64cd603f-353e-4ea6-9dff-2ec94dc9ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction types:\n",
      "Class predictions shape: (114,)\n",
      "Probability predictions shape (114, 2)\n",
      "\n",
      "First 10 predictions\n",
      "Actual | Predicted | Prob(Benign) | Prob(Malignant)\n",
      "--------------------------------------------------\n",
      "Malignant | Malignant |       0.000 |         1.000\n",
      "Benign   | Benign    |       1.000 |         0.000\n",
      "Malignant | Malignant |       0.050 |         0.950\n",
      "Benign   | Benign    |       0.604 |         0.396\n",
      "Malignant | Malignant |       0.000 |         1.000\n",
      "Benign   | Benign    |       0.983 |         0.017\n",
      "Benign   | Benign    |       1.000 |         0.000\n",
      "Malignant | Malignant |       0.000 |         1.000\n",
      "Malignant | Malignant |       0.000 |         1.000\n",
      "Malignant | Malignant |       0.000 |         1.000\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "print(\"Prediction types:\")\n",
    "print(f\"Class predictions shape: {y_pred.shape}\")\n",
    "print(f\"Probability predictions shape {y_prob.shape}\")\n",
    "\n",
    "# Show the first 10 predictions\n",
    "print(\"\\nFirst 10 predictions\")\n",
    "print(f\"Actual | Predicted | Prob(Benign) | Prob(Malignant)\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(10):\n",
    "    actual = \"Benign\" if y_test[i] == 1 else \"Malignant\"\n",
    "    predicted = \"Benign\" if y_pred[i] == 1 else \"Malignant\"\n",
    "    prob_benign = y_prob[i][1]\n",
    "    prob_malignant = y_prob[i][0]\n",
    "    print(f\"{actual:8} | {predicted:9} | {prob_benign:11.3f} | {prob_malignant:13.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c2d44-0f8f-4776-8725-14fb27f54e2e",
   "metadata": {},
   "source": [
    "### 1.4 Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c52dca57-1cdf-41c8-bd57-4942eda5e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Accuracy: 0.965 (96.5)\n",
      "\n",
      "Detailed Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.97      0.93      0.95        42\n",
      "      benign       0.96      0.99      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "worst concavity: -1.316 decreases malignant probability\n",
      "texture error: 1.092 increases malignant probability\n",
      "mean radius: 0.806 increases malignant probability\n",
      "worst symmetry: -0.782 decreases malignant probability\n",
      "worst compactness: -0.754 decreases malignant probability\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.3f} ({accuracy*100:.1f})\")\n",
    "\n",
    "print(\"\\nDetailed Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "\n",
    "feature_names = data.feature_names\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients\n",
    "})\n",
    "feature_importance['abs_coef'] = abs(feature_importance['coefficient'])\n",
    "top_features = feature_importance.nlargest(5, 'abs_coef')\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for _, row in top_features.iterrows():\n",
    "    direction = \"increases\" if row['coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"{row['feature']}: {row['coefficient']:.3f} {direction} malignant probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5900e-b3dc-4ad4-bfd9-f37cdc179a77",
   "metadata": {},
   "source": [
    "## 2: Data Preprocessing & Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c545a9d-4726-43ea-a0cb-7bc99b44800a",
   "metadata": {},
   "source": [
    "### 2.1 Why Scaling Matters - Demonstrating the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54cecd28-01f6-4d8a-99db-5bdfa94a5ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Scale Analysis:\n",
      "Features with HUGE scale differences:\n",
      "              feature     min     max    range\n",
      "12            proline  278.00  1680.0  1402.00\n",
      "4           magnesium   70.00   162.0    92.00\n",
      "3   alcalinity_of_ash   10.60    30.0    19.40\n",
      "9     color_intensity    1.28    13.0    11.72\n",
      "1          malic_acid    0.74     5.8     5.06\n",
      "\n",
      "Features with small scales:\n",
      "                         feature   min   max  range\n",
      "7           nonflavanoid_phenols  0.13  0.66   0.53\n",
      "10                           hue  0.48  1.71   1.23\n",
      "2                            ash  1.36  3.23   1.87\n",
      "11  od280/od315_of_diluted_wines  1.27  4.00   2.73\n",
      "5                  total_phenols  0.98  3.88   2.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # To hide convergence warning\n",
    "\n",
    "data = load_wine()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "y_binary = (y==0).astype(int)\n",
    "\n",
    "feature_scales = pd.DataFrame({\n",
    "    'feature': data.feature_names,\n",
    "    'min': X.min(axis=0),\n",
    "    'max': X.max(axis=0),\n",
    "    'range': X.max(axis=0) - X.min(axis=0)\n",
    "}).round(2)\n",
    "\n",
    "print(\"Feature Scale Analysis:\")\n",
    "print(\"Features with HUGE scale differences:\")\n",
    "print(feature_scales.nlargest(5, 'range')[['feature', 'min', 'max', 'range']])\n",
    "print(\"\\nFeatures with small scales:\")\n",
    "print(feature_scales.nsmallest(5, 'range')[['feature', 'min', 'max', 'range']])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4c739-3230-4e5e-9fe5-83d8e72a3622",
   "metadata": {},
   "source": [
    "### 2.2 Training Without Scaling - See the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "161fa0a5-34d4-4905-988a-a3be5e2119d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WITHOUT scaling:\n",
      "Converged: False\n",
      "Iterations used: 100/100\n",
      "Accuracy without scaling: 1.000\n",
      "\n",
      "Coefficient range: -0.570768 to 1.222616\n",
      "Problem: Coefficients vary wildly due to scale differences!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Training WITHOUT scaling:\")\n",
    "model_unscaled = LogisticRegression(random_state=42, max_iter=100)\n",
    "model_unscaled.fit(X_train, y_train)\n",
    "\n",
    "# Checking for convergence\n",
    "print(f\"Converged: {model_unscaled.n_iter_[0] < 100}\")\n",
    "print(f\"Iterations used: {model_unscaled.n_iter_[0]}/100\")\n",
    "\n",
    "# Checking performance\n",
    "y_pred_unscaled = model_unscaled.predict(X_test)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "print(f\"Accuracy without scaling: {accuracy_unscaled:.3f}\")\n",
    "\n",
    "# Checking the coefficients\n",
    "coef_unscaled = model_unscaled.coef_[0]\n",
    "print(f\"\\nCoefficient range: {coef_unscaled.min():.6f} to {coef_unscaled.max():.6f}\")\n",
    "print(\"Problem: Coefficients vary wildly due to scale differences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195f813-b87a-4654-bcbe-02577783d544",
   "metadata": {},
   "source": [
    "### 2.3 Training With StandardScaler - The Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9681c349-ad23-44e0-a04e-a6680e7e9da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WITH StandardScaler:\n",
      "After scaling - feature statistics:\n",
      "Mean: [4.35957999e-15 1.10475009e-15 2.02576610e-15]\n",
      "Std:  [1. 1. 1.]\n",
      "\n",
      "Converged: True\n",
      "Iterations used: 12/100\n",
      "Accuracy with scaling: 0.972\n",
      "\n",
      "Coefficient range: -1.096 to 1.784\n",
      "Much better! Coefficients are now comparable in magnitude\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Training WITH StandardScaler:\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform test data with same scaler and avoiding data leakage\n",
    "\n",
    "print(\"After scaling - feature statistics:\")\n",
    "print(f\"Mean: {X_train_scaled.mean(axis=0)[:3]}\")  # Should be ~0\n",
    "print(f\"Std:  {X_train_scaled.std(axis=0)[:3]}\")   # Should be ~1\n",
    "\n",
    "# Train model on scaled data\n",
    "model_scaled = LogisticRegression(random_state=42, max_iter=100)  # Same low max_iter\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nConverged: {model_scaled.n_iter_[0] < 100}\")\n",
    "print(f\"Iterations used: {model_scaled.n_iter_[0]}/100\")\n",
    "\n",
    "# Performance\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f\"Accuracy with scaling: {accuracy_scaled:.3f}\")\n",
    "\n",
    "# Coefficients are now comparable\n",
    "coef_scaled = model_scaled.coef_[0]\n",
    "print(f\"\\nCoefficient range: {coef_scaled.min():.3f} to {coef_scaled.max():.3f}\")\n",
    "print(\"Much better! Coefficients are now comparable in magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c948d62-9e83-441e-99cf-744d81dfc04e",
   "metadata": {},
   "source": [
    "### 2.4 Impact Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e08aea6-aa5f-4255-8e37-16aa48a660a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCALING IMPACT COMPARISON ===\n",
      "Metric               Without Scaling With Scaling    Improvement\n",
      "-----------------------------------------------------------------\n",
      "Convergence          False           True            ✓\n",
      "Iterations           100             12              +88\n",
      "Accuracy             1.000           0.972           -0.028\n",
      "Coef Range           1.793           2.880           +1.087\n"
     ]
    }
   ],
   "source": [
    "# Direct comparison\n",
    "print(\"=== SCALING IMPACT COMPARISON ===\")\n",
    "print(f\"{'Metric':<20} {'Without Scaling':<15} {'With Scaling':<15} {'Improvement'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "metrics = {\n",
    "    'Convergence': [model_unscaled.n_iter_[0] < 100, model_scaled.n_iter_[0] < 100],\n",
    "    'Iterations': [model_unscaled.n_iter_[0], model_scaled.n_iter_[0]],\n",
    "    'Accuracy': [accuracy_unscaled, accuracy_scaled],\n",
    "    'Coef Range': [coef_unscaled.max() - coef_unscaled.min(), \n",
    "                   coef_scaled.max() - coef_scaled.min()]\n",
    "}\n",
    "\n",
    "for metric, values in metrics.items():\n",
    "    if metric == 'Convergence':\n",
    "        print(f\"{metric:<20} {str(values[0]):<15} {str(values[1]):<15} {'✓' if values[1] else '✗'}\")\n",
    "    elif metric == 'Iterations':\n",
    "        improvement = f\"{values[0] - values[1]:+d}\"\n",
    "        print(f\"{metric:<20} {values[0]:<15} {values[1]:<15} {improvement}\")\n",
    "    else:\n",
    "        improvement = f\"{values[1] - values[0]:+.3f}\"\n",
    "        print(f\"{metric:<20} {values[0]:<15.3f} {values[1]:<15.3f} {improvement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de472f7a-0380-47b5-a63e-f073d3905a4a",
   "metadata": {},
   "source": [
    "## 3. Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7056942-f461-429b-9555-e73afb8e5b3b",
   "metadata": {},
   "source": [
    "### 3.1 Detecting Imbalance - The Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f418334e-a315-4881-b101-1d7d09ba4cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Imbalance Analysis:\n",
      "Class 0: 9,456 samples (94.6%)\n",
      "Class 1: 544 samples (5.4%)\n",
      "\n",
      "Dataset: 10,000 transactions\n",
      "Fraud rate: 5.4% (highly imbalanced!)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate imbalanced data: 5% fraud, 95% normal transactions\n",
    "X, y = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=3,\n",
    "    weights=[0.95, 0.05],  # 95% class 0, 5% class 1\n",
    "    flip_y=0.01,           # Add some noise\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Class Imbalance Analysis:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for class_label, count in zip(unique, counts):\n",
    "    percentage = count / len(y) * 100\n",
    "    print(f\"Class {class_label}: {count:,} samples ({percentage:.1f}%)\")\n",
    "\n",
    "class_names = ['Normal', 'Fraud']\n",
    "print(f\"\\nDataset: {X.shape[0]:,} transactions\")\n",
    "print(f\"Fraud rate: {np.mean(y):.1%} (highly imbalanced!)\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2bc03-535d-43a1-a5b8-584f0f7aa9c7",
   "metadata": {},
   "source": [
    "### 3.2 Why Accuracy Fails - Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad961e03-0c83-48fc-b99d-bdb6ff50ebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASIC MODEL (No Class Balancing) ===\n",
      "Accuracy: 0.949 (94.9%)\n",
      "\n",
      "Detailed Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.95      1.00      0.97      1891\n",
      "       Fraud       0.73      0.10      0.18       109\n",
      "\n",
      "    accuracy                           0.95      2000\n",
      "   macro avg       0.84      0.55      0.58      2000\n",
      "weighted avg       0.94      0.95      0.93      2000\n",
      "\n",
      "\n",
      "THE PROBLEM:\n",
      "Fraud cases detected: 11/109\n",
      "Fraud detection rate: 10.1%\n",
      "Model might just predict 'Normal' for everything and still get 95% accuracy!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features (always needed for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train basic model (no class balancing)\n",
    "model_basic = LogisticRegression(random_state=42)\n",
    "model_basic.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate with accuracy only\n",
    "y_pred_basic = model_basic.predict(X_test_scaled)\n",
    "accuracy_basic = accuracy_score(y_test, y_pred_basic)\n",
    "\n",
    "print(\"=== BASIC MODEL (No Class Balancing) ===\")\n",
    "print(f\"Accuracy: {accuracy_basic:.3f} ({accuracy_basic*100:.1f}%)\")\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(classification_report(y_test, y_pred_basic, target_names=class_names))\n",
    "\n",
    "# Show the problem with accuracy\n",
    "fraud_detected = np.sum((y_test == 1) & (y_pred_basic == 1))\n",
    "total_fraud = np.sum(y_test == 1)\n",
    "print(f\"\\nTHE PROBLEM:\")\n",
    "print(f\"Fraud cases detected: {fraud_detected}/{total_fraud}\")\n",
    "print(f\"Fraud detection rate: {fraud_detected/total_fraud:.1%}\")\n",
    "print(f\"Model might just predict 'Normal' for everything and still get 95% accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790906f-25b5-4885-99ba-0b5d02500f55",
   "metadata": {},
   "source": [
    "### 3.3 Better Metrics - Understanding What Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "180ce35a-d56f-4069-a18c-92a65a4ff3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNDERSTANDING METRICS FOR IMBALANCED DATA ===\n",
      "Accuracy: 0.949\n",
      "Precision: 0.733\n",
      "Recall: 0.101\n",
      "F1-Score: 0.177\n",
      "ROC-AUC: 0.810\n",
      "\n",
      "📚 METRIC EXPLANATIONS:\n",
      "• Accuracy: Overall correct predictions (misleading with imbalance)\n",
      "• Precision: Of predicted fraud, how many were actually fraud?\n",
      "• Recall: Of actual fraud, how many did we catch?\n",
      "• F1-Score: Harmonic mean of precision and recall\n",
      "• ROC-AUC: How well can model distinguish between classes?\n",
      "\n",
      "📊 CONFUSION MATRIX BREAKDOWN:\n",
      "True Negatives (Normal correctly): 1887\n",
      "False Positives (Normal as Fraud): 4\n",
      "False Negatives (Fraud as Normal): 98 ⚠️ BAD!\n",
      "True Positives (Fraud correctly): 11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Calculate better metrics for imbalanced data\n",
    "y_prob_basic = model_basic.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "metrics_basic = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_basic),\n",
    "    'Precision': precision_score(y_test, y_pred_basic),\n",
    "    'Recall': recall_score(y_test, y_pred_basic),\n",
    "    'F1-Score': f1_score(y_test, y_pred_basic),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_prob_basic)\n",
    "}\n",
    "\n",
    "print(\"=== UNDERSTANDING METRICS FOR IMBALANCED DATA ===\")\n",
    "for metric, value in metrics_basic.items():\n",
    "    print(f\"{metric}: {value:.3f}\")\n",
    "\n",
    "print(f\"\\n📚 METRIC EXPLANATIONS:\")\n",
    "print(f\"• Accuracy: Overall correct predictions (misleading with imbalance)\")\n",
    "print(f\"• Precision: Of predicted fraud, how many were actually fraud?\")\n",
    "print(f\"• Recall: Of actual fraud, how many did we catch?\")\n",
    "print(f\"• F1-Score: Harmonic mean of precision and recall\")\n",
    "print(f\"• ROC-AUC: How well can model distinguish between classes?\")\n",
    "\n",
    "# Show confusion matrix breakdown\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_basic)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\n📊 CONFUSION MATRIX BREAKDOWN:\")\n",
    "print(f\"True Negatives (Normal correctly): {tn}\")\n",
    "print(f\"False Positives (Normal as Fraud): {fp}\")  \n",
    "print(f\"False Negatives (Fraud as Normal): {fn} ⚠️ BAD!\")\n",
    "print(f\"True Positives (Fraud correctly): {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b76af-2403-42df-b00d-1fa5d98b9f6e",
   "metadata": {},
   "source": [
    "### 3.4 Class Weights - The Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93e89e55-ac4a-4af8-956a-adf834bcd204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARISON: BASIC vs BALANCED ===\n",
      "Metric       Basic    Balanced   Change\n",
      "----------------------------------------\n",
      "Accuracy     0.949    0.761      -0.188\n",
      "Precision    0.733    0.157      -0.577\n",
      "Recall       0.101    0.771      +0.670\n",
      "F1-Score     0.177    0.260      +0.083\n",
      "ROC-AUC      0.810    0.830      +0.020\n",
      "\n",
      "⚖️ CLASS WEIGHTS EXPLANATION:\n",
      "Normal class weight: 0.529\n",
      "Fraud class weight: 9.195\n",
      "Fraud gets 17.4x more weight in training\n"
     ]
    }
   ],
   "source": [
    "# Train model with balanced class weights\n",
    "model_balanced = LogisticRegression(\n",
    "    class_weight='balanced',  # Automatically balances classes\n",
    "    random_state=42\n",
    ")\n",
    "model_balanced.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compare results\n",
    "y_pred_balanced = model_balanced.predict(X_test_scaled)\n",
    "y_prob_balanced = model_balanced.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "metrics_balanced = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_balanced),\n",
    "    'Precision': precision_score(y_test, y_pred_balanced),\n",
    "    'Recall': recall_score(y_test, y_pred_balanced),\n",
    "    'F1-Score': f1_score(y_test, y_pred_balanced),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_prob_balanced)\n",
    "}\n",
    "\n",
    "print(\"=== COMPARISON: BASIC vs BALANCED ===\")\n",
    "print(f\"{'Metric':<12} {'Basic':<8} {'Balanced':<10} {'Change'}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for metric in metrics_basic.keys():\n",
    "    basic_val = metrics_basic[metric]\n",
    "    balanced_val = metrics_balanced[metric]\n",
    "    change = balanced_val - basic_val\n",
    "    change_str = f\"{change:+.3f}\"\n",
    "    print(f\"{metric:<12} {basic_val:<8.3f} {balanced_val:<10.3f} {change_str}\")\n",
    "\n",
    "# Show what class_weight='balanced' does\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "print(f\"\\n⚖️ CLASS WEIGHTS EXPLANATION:\")\n",
    "print(f\"Normal class weight: {class_weights[0]:.3f}\")\n",
    "print(f\"Fraud class weight: {class_weights[1]:.3f}\")\n",
    "print(f\"Fraud gets {class_weights[1]/class_weights[0]:.1f}x more weight in training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194dbab9-7e5f-45f4-84a5-63fb343174df",
   "metadata": {},
   "source": [
    "### 3.5 Custom Threshold Tuning - Business Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0725d328-a648-4497-be27-6a3cb56851b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== THRESHOLD OPTIMIZATION ===\n",
      "Default threshold: 0.5\n",
      "Optimal F1 threshold: 0.826\n",
      "\n",
      "🎯 BUSINESS SCENARIO COMPARISON:\n",
      "Scenario                  Threshold   Precision  Recall   F1\n",
      "-----------------------------------------------------------------\n",
      "Conservative (High Precision) 0.800       0.327      0.339    0.333\n",
      "Aggressive (High Recall)  0.300       0.095      0.890    0.172\n",
      "Balanced F1               0.826       0.391      0.312    0.347\n",
      "\n",
      "💡 BUSINESS INTERPRETATION:\n",
      "• Conservative: Minimize false alarms, might miss some fraud\n",
      "• Aggressive: Catch most fraud, but more false alarms\n",
      "• Balanced: Best overall trade-off between precision and recall\n"
     ]
    }
   ],
   "source": [
    "# Sometimes you need custom thresholds for business needs\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find optimal threshold for different business objectives\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_prob_balanced)\n",
    "\n",
    "# Calculate F1 scores for all thresholds\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "optimal_threshold_f1 = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(\"=== THRESHOLD OPTIMIZATION ===\")\n",
    "print(f\"Default threshold: 0.5\")\n",
    "print(f\"Optimal F1 threshold: {optimal_threshold_f1:.3f}\")\n",
    "\n",
    "# Test different business scenarios\n",
    "scenarios = {\n",
    "    'Conservative (High Precision)': 0.8,  # Only flag if very confident\n",
    "    'Aggressive (High Recall)': 0.3,      # Flag more cases to catch fraud\n",
    "    'Balanced F1': optimal_threshold_f1    # Optimal F1 score\n",
    "}\n",
    "\n",
    "print(f\"\\n🎯 BUSINESS SCENARIO COMPARISON:\")\n",
    "print(f\"{'Scenario':<25} {'Threshold':<11} {'Precision':<10} {'Recall':<8} {'F1'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for scenario, threshold in scenarios.items():\n",
    "    y_pred_custom = (y_prob_balanced >= threshold).astype(int)\n",
    "    prec = precision_score(y_test, y_pred_custom)\n",
    "    rec = recall_score(y_test, y_pred_custom)\n",
    "    f1 = f1_score(y_test, y_pred_custom)\n",
    "    \n",
    "    print(f\"{scenario:<25} {threshold:<11.3f} {prec:<10.3f} {rec:<8.3f} {f1:.3f}\")\n",
    "\n",
    "print(f\"\\n💡 BUSINESS INTERPRETATION:\")\n",
    "print(f\"• Conservative: Minimize false alarms, might miss some fraud\")\n",
    "print(f\"• Aggressive: Catch most fraud, but more false alarms\")\n",
    "print(f\"• Balanced: Best overall trade-off between precision and recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade142a-e637-4aa1-b6d9-28c7ff806a7b",
   "metadata": {},
   "source": [
    "## 4. Regularization & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de913c1a-af43-4357-9a3c-dd773649bd2a",
   "metadata": {},
   "source": [
    "### 4.1 L1 vs L2 Regularization - Understanding the Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4789aec-0aa9-4dd0-8cd3-761c1ca48dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Dimensional Dataset:\n",
      "Total features: 50\n",
      "Informative features: 10\n",
      "Irrelevant features: 40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Dataset with many features (some irrelevant)\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=50,        # 50 features total\n",
    "    n_informative=10,     # Only 10 are actually useful\n",
    "    n_redundant=5,        # 5 are redundant\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"High-Dimensional Dataset:\")\n",
    "print(f\"Total features: {X.shape[1]}\")\n",
    "print(f\"Informative features: 10\")\n",
    "print(f\"Irrelevant features: {50 - 10}\")\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa2ad38-5128-4c0a-b2f3-c033b35f84c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REGULARIZATION COMPARISON ===\n",
      "Model              Train Acc  Test Acc  ROC-AUC  Non-Zero Coef\n",
      "------------------------------------------------------------\n",
      "No Regularization  0.966      0.950     0.995    50\n",
      "L1 (Lasso)         0.963      0.965     0.997    9\n",
      "L2 (Ridge)         0.965      0.965     0.997    50\n",
      "\n",
      "🎯 OVERFITTING ANALYSIS:\n",
      "No Regularization: 0.016 (lower is better)\n",
      "L1 (Lasso): -0.002 (lower is better)\n",
      "L2 (Ridge): 0.000 (lower is better)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Compare different regularization approaches\n",
    "models = {\n",
    "    'No Regularization': LogisticRegression(C=1e6, random_state=42, max_iter=2000),  # Very high C = almost no regularization\n",
    "    'L1 (Lasso)': LogisticRegression(penalty='l1', C=0.1, solver='liblinear', random_state=42),\n",
    "    'L2 (Ridge)': LogisticRegression(penalty='l2', C=0.1, random_state=42, max_iter=2000)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "print(\"=== REGULARIZATION COMPARISON ===\")\n",
    "print(f\"{'Model':<18} {'Train Acc':<10} {'Test Acc':<9} {'ROC-AUC':<8} {'Non-Zero Coef'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train_scaled))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "    test_auc = roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:, 1])\n",
    "    \n",
    "    # Count non-zero coefficients (feature selection)\n",
    "    non_zero_coef = np.sum(np.abs(model.coef_[0]) > 0.001)\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'auc': test_auc,\n",
    "        'non_zero_coef': non_zero_coef,\n",
    "        'coefficients': model.coef_[0]\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:<18} {train_acc:<10.3f} {test_acc:<9.3f} {test_auc:<8.3f} {non_zero_coef}\")\n",
    "\n",
    "# Show overfitting\n",
    "print(f\"\\n🎯 OVERFITTING ANALYSIS:\")\n",
    "for name, result in results.items():\n",
    "    overfitting = result['train_acc'] - result['test_acc']\n",
    "    print(f\"{name}: {overfitting:.3f} (lower is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff6972-c9be-4b7a-8324-12105e3307ce",
   "metadata": {},
   "source": [
    "### 4.3 Understanding the C Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b6176d9-f841-4558-abe3-6027921eaba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== C PARAMETER IMPACT (L2 Regularization) ===\n",
      "C Value  Train Acc  Test Acc  Overfitting Non-Zero Coef\n",
      "-------------------------------------------------------\n",
      "0.001    0.949      0.950     -0.001      47\n",
      "0.01     0.960      0.960     0.000       50\n",
      "0.1      0.965      0.965     0.000       50\n",
      "1.0      0.963      0.955     0.008       50\n",
      "10.0     0.964      0.950     0.014       50\n",
      "100.0    0.966      0.950     0.016       50\n",
      "\n",
      "💡 C PARAMETER INTERPRETATION:\n",
      "• High C (100): Less regularization → More complex model → Potential overfitting\n",
      "• Low C (0.001): More regularization → Simpler model → Potential underfitting\n",
      "• Sweet spot: Usually between 0.1 and 10.0\n",
      "• Best C from our test: 0.001 (lowest overfitting)\n"
     ]
    }
   ],
   "source": [
    "# Test different C values to understand regularization strength\n",
    "C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "print(\"=== C PARAMETER IMPACT (L2 Regularization) ===\")\n",
    "print(f\"{'C Value':<8} {'Train Acc':<10} {'Test Acc':<9} {'Overfitting':<11} {'Non-Zero Coef'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "c_results = {}\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(penalty='l2', C=C, random_state=42, max_iter=2000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train_scaled))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "    overfitting = train_acc - test_acc\n",
    "    non_zero_coef = np.sum(np.abs(model.coef_[0]) > 0.001)\n",
    "    \n",
    "    c_results[C] = {'train': train_acc, 'test': test_acc, 'overfitting': overfitting}\n",
    "    \n",
    "    print(f\"{C:<8} {train_acc:<10.3f} {test_acc:<9.3f} {overfitting:<11.3f} {non_zero_coef}\")\n",
    "\n",
    "print(f\"\\n💡 C PARAMETER INTERPRETATION:\")\n",
    "print(f\"• High C (100): Less regularization → More complex model → Potential overfitting\")\n",
    "print(f\"• Low C (0.001): More regularization → Simpler model → Potential underfitting\")\n",
    "print(f\"• Sweet spot: Usually between 0.1 and 10.0\")\n",
    "\n",
    "# Find best C from our test\n",
    "best_c = min(c_results.keys(), key=lambda x: c_results[x]['overfitting'])\n",
    "print(f\"• Best C from our test: {best_c} (lowest overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b4cac-4a44-4f33-90dd-85ca9b5eca52",
   "metadata": {},
   "source": [
    "### 4.4 GridSearchCV - Systematic Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "327c8012-1165-4782-b337-50c569437592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRIDSEARCHCV FOR OPTIMAL HYPERPARAMETERS ===\n",
      "Searching through parameter combinations...\n",
      "Best parameters: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best cross-validation AUC: 0.990\n",
      "Test set performance:\n",
      "• Test AUC: 0.997\n",
      "• Test Accuracy: 0.965\n",
      "• Features selected by L1: 9/50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']  # Works with both L1 and L2\n",
    "}\n",
    "\n",
    "print(\"=== GRIDSEARCHCV FOR OPTIMAL HYPERPARAMETERS ===\")\n",
    "print(\"Searching through parameter combinations...\")\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=42, max_iter=2000),\n",
    "    param_grid,\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring='roc_auc',       # Use AUC for imbalanced data\n",
    "    n_jobs=-1,               # Use all CPU cores\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation AUC: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate best model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "test_acc = accuracy_score(y_test, best_model.predict(X_test_scaled))\n",
    "\n",
    "print(f\"Test set performance:\")\n",
    "print(f\"• Test AUC: {test_auc:.3f}\")\n",
    "print(f\"• Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# Show feature selection with best model\n",
    "if grid_search.best_params_['penalty'] == 'l1':\n",
    "    selected_features = np.sum(np.abs(best_model.coef_[0]) > 0.001)\n",
    "    print(f\"• Features selected by L1: {selected_features}/{X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25efffb-652e-48ae-9d27-b51f8ce2f1a7",
   "metadata": {},
   "source": [
    "### 4.5 Cross-Validation - Reliable Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "854443b1-d6d4-4a03-aec8-1eaab576c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CROSS-VALIDATION FOR RELIABLE EVALUATION ===\n",
      "\n",
      "Baseline (no reg):\n",
      "• CV Accuracy: 0.941 ± 0.017\n",
      "• CV ROC-AUC: 0.986 ± 0.008\n",
      "• CV Precision: 0.939 ± 0.028\n",
      "• CV Recall: 0.945 ± 0.010\n",
      "• Overfitting (Train-CV): 0.010\n",
      "\n",
      "Best from Grid:\n",
      "• CV Accuracy: 0.963 ± 0.017\n",
      "• CV ROC-AUC: 0.990 ± 0.007\n",
      "• CV Precision: 0.956 ± 0.026\n",
      "• CV Recall: 0.970 ± 0.013\n",
      "• Overfitting (Train-CV): 0.001\n",
      "\n",
      "📊 WHY CROSS-VALIDATION MATTERS:\n",
      "• Single train/test split can be lucky or unlucky\n",
      "• CV uses multiple splits for reliable estimates\n",
      "• Standard deviation shows consistency across folds\n",
      "• Compare train vs CV scores to detect overfitting\n",
      "\n",
      "🏆 FINAL MODEL SELECTED:\n",
      "• Algorithm: LogisticRegression\n",
      "• Penalty: l1\n",
      "• C: 0.1\n",
      "• Expected AUC: 0.990\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Demonstrate proper cross-validation\n",
    "print(\"=== CROSS-VALIDATION FOR RELIABLE EVALUATION ===\")\n",
    "\n",
    "# Compare our best model with baseline\n",
    "models_to_compare = {\n",
    "    'Baseline (no reg)': LogisticRegression(C=1e6, random_state=42, max_iter=2000),\n",
    "    'Best from Grid': grid_search.best_estimator_\n",
    "}\n",
    "\n",
    "cv_scores = {}\n",
    "for name, model in models_to_compare.items():\n",
    "    # Multiple metrics with cross-validation\n",
    "    scores = cross_validate(\n",
    "        model, X_train_scaled, y_train,\n",
    "        cv=5,\n",
    "        scoring=['accuracy', 'roc_auc', 'precision', 'recall'],\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    cv_scores[name] = scores\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"• CV Accuracy: {scores['test_accuracy'].mean():.3f} ± {scores['test_accuracy'].std():.3f}\")\n",
    "    print(f\"• CV ROC-AUC: {scores['test_roc_auc'].mean():.3f} ± {scores['test_roc_auc'].std():.3f}\")\n",
    "    print(f\"• CV Precision: {scores['test_precision'].mean():.3f} ± {scores['test_precision'].std():.3f}\")\n",
    "    print(f\"• CV Recall: {scores['test_recall'].mean():.3f} ± {scores['test_recall'].std():.3f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    train_auc = scores['train_roc_auc'].mean()\n",
    "    test_auc = scores['test_roc_auc'].mean()\n",
    "    overfitting = train_auc - test_auc\n",
    "    print(f\"• Overfitting (Train-CV): {overfitting:.3f}\")\n",
    "\n",
    "print(f\"\\n📊 WHY CROSS-VALIDATION MATTERS:\")\n",
    "print(f\"• Single train/test split can be lucky or unlucky\")\n",
    "print(f\"• CV uses multiple splits for reliable estimates\")\n",
    "print(f\"• Standard deviation shows consistency across folds\")\n",
    "print(f\"• Compare train vs CV scores to detect overfitting\")\n",
    "\n",
    "# Final model selection\n",
    "final_model = grid_search.best_estimator_\n",
    "print(f\"\\n🏆 FINAL MODEL SELECTED:\")\n",
    "print(f\"• Algorithm: LogisticRegression\")\n",
    "print(f\"• Penalty: {grid_search.best_params_['penalty']}\")\n",
    "print(f\"• C: {grid_search.best_params_['C']}\")\n",
    "print(f\"• Expected AUC: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2300af-d4d6-42b9-a560-a273ea199073",
   "metadata": {},
   "source": [
    "## Section 5 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e0c12-06f1-437f-883b-cec4b5a5b25d",
   "metadata": {},
   "source": [
    "### 5.1 Categorical Encoding - Handling Non-Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73895e91-e3a2-45f0-8fb6-16ac6b9e35e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MIXED DATA TYPES DATASET ===\n",
      "Dataset shape: (2000, 7)\n",
      "Churn rate: 53.5%\n",
      "\n",
      "Data types:\n",
      "age                   int32\n",
      "income              float64\n",
      "months_tenure       float64\n",
      "contract_type        object\n",
      "payment_method       object\n",
      "internet_service     object\n",
      "churned               int32\n",
      "dtype: object\n",
      "\n",
      "Categorical variables:\n",
      "• contract_type: 3 categories ['annual', 'monthly', 'two_year']\n",
      "• payment_method: 3 categories ['bank_transfer', 'credit_card', 'electronic_check']\n",
      "• internet_service: 3 categories ['dsl', 'fiber', 'no']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Customer churn dataset with mixed feature types\n",
    "np.random.seed(42)\n",
    "n_customers = 2000\n",
    "\n",
    "# Create realistic customer data\n",
    "data = {\n",
    "    'age': np.random.normal(40, 15, n_customers).astype(int),\n",
    "    'income': np.random.normal(50000, 20000, n_customers),\n",
    "    'months_tenure': np.random.uniform(1, 60, n_customers),\n",
    "    'contract_type': np.random.choice(['monthly', 'annual', 'two_year'], n_customers, p=[0.5, 0.3, 0.2]),\n",
    "    'payment_method': np.random.choice(['credit_card', 'bank_transfer', 'electronic_check'], n_customers, p=[0.4, 0.3, 0.3]),\n",
    "    'internet_service': np.random.choice(['dsl', 'fiber', 'no'], n_customers, p=[0.4, 0.4, 0.2])\n",
    "}\n",
    "\n",
    "# Create logical churn patterns\n",
    "churn_probability = (\n",
    "    -0.02 * data['age'] +                    # Younger customers churn more\n",
    "    -0.00001 * data['income'] +              # Higher income customers churn less\n",
    "    -0.05 * data['months_tenure'] +          # Longer tenure = less churn\n",
    "    (np.array(data['contract_type']) == 'monthly') * 1.5 +  # Monthly contracts churn more\n",
    "    (np.array(data['payment_method']) == 'electronic_check') * 0.8 +  # Electronic check risky\n",
    "    2.0  # Base churn rate\n",
    ")\n",
    "\n",
    "# Convert to probability and binary outcome\n",
    "churn_prob = 1 / (1 + np.exp(-churn_probability))\n",
    "data['churned'] = np.random.binomial(1, churn_prob)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"=== MIXED DATA TYPES DATASET ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Churn rate: {df['churned'].mean():.1%}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nCategorical variables:\")\n",
    "categorical_cols = ['contract_type', 'payment_method', 'internet_service']\n",
    "for col in categorical_cols:\n",
    "    print(f\"• {col}: {df[col].nunique()} categories {list(df[col].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb5061-542b-48ee-bdd3-a5e0a27101b5",
   "metadata": {},
   "source": [
    "### 5.2 One-Hot Encoding with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "659efb6c-e678-4b84-b331-469d82b7137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE ONE-HOT ENCODING ===\n",
      "AUC with numeric features only: 0.669\n",
      "\n",
      "=== AFTER ONE-HOT ENCODING ===\n",
      "Original features: 7\n",
      "After encoding: 13\n",
      "New categorical features created:\n",
      "• contract_type → ['contract_type_annual', 'contract_type_monthly', 'contract_type_two_year']\n",
      "• payment_method → ['payment_method_bank_transfer', 'payment_method_credit_card', 'payment_method_electronic_check']\n",
      "• internet_service → ['internet_service_dsl', 'internet_service_fiber', 'internet_service_no']\n",
      "\n",
      "AUC with categorical features: 0.770\n",
      "Improvement: +0.101\n"
     ]
    }
   ],
   "source": [
    "# Before encoding - show the problem\n",
    "print(\"=== BEFORE ONE-HOT ENCODING ===\")\n",
    "numeric_cols = ['age', 'income', 'months_tenure']\n",
    "X_numeric_only = df[numeric_cols]\n",
    "y = df['churned']\n",
    "\n",
    "# Try training with numeric features only\n",
    "X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
    "    X_numeric_only, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
    "X_test_num_scaled = scaler.transform(X_test_num)\n",
    "\n",
    "model_numeric = LogisticRegression(random_state=42)\n",
    "model_numeric.fit(X_train_num_scaled, y_train)\n",
    "\n",
    "auc_numeric = roc_auc_score(y_test, model_numeric.predict_proba(X_test_num_scaled)[:, 1])\n",
    "print(f\"AUC with numeric features only: {auc_numeric:.3f}\")\n",
    "\n",
    "# One-hot encoding\n",
    "print(f\"\\n=== AFTER ONE-HOT ENCODING ===\")\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols)\n",
    "\n",
    "print(f\"Original features: {len(df.columns)}\")\n",
    "print(f\"After encoding: {len(df_encoded.columns)}\")\n",
    "print(f\"New categorical features created:\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    new_cols = [c for c in df_encoded.columns if c.startswith(col)]\n",
    "    print(f\"• {col} → {new_cols}\")\n",
    "\n",
    "# Train with all features\n",
    "X_encoded = df_encoded.drop('churned', axis=1)\n",
    "X_train_enc, X_test_enc, _, _ = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler_enc = StandardScaler()\n",
    "X_train_enc_scaled = scaler_enc.fit_transform(X_train_enc)\n",
    "X_test_enc_scaled = scaler_enc.transform(X_test_enc)\n",
    "\n",
    "model_encoded = LogisticRegression(random_state=42)\n",
    "model_encoded.fit(X_train_enc_scaled, y_train)\n",
    "\n",
    "auc_encoded = roc_auc_score(y_test, model_encoded.predict_proba(X_test_enc_scaled)[:, 1])\n",
    "print(f\"\\nAUC with categorical features: {auc_encoded:.3f}\")\n",
    "print(f\"Improvement: {auc_encoded - auc_numeric:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa35c89-ef47-4910-93ee-1256d525338a",
   "metadata": {},
   "source": [
    "### 5.3 Polynomial Features - Creating Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25f5dbb7-8e08-4eb2-9574-4c33fcb33c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POLYNOMIAL FEATURES FOR INTERACTIONS ===\n",
      "Original features:\n",
      "['age', 'income', 'months_tenure']\n",
      "\n",
      "After polynomial features (degree=2):\n",
      "Feature count: 3 → 9\n",
      "New features created:\n",
      "• age^2\n",
      "• age income\n",
      "• age months_tenure\n",
      "• income^2\n",
      "• income months_tenure\n",
      "• months_tenure^2\n",
      "\n",
      "Performance comparison:\n",
      "• Numeric only: 0.669\n",
      "• With categoricals: 0.770\n",
      "• With polynomials: 0.669\n",
      "\n",
      "Top 5 most important polynomial features:\n",
      "                feature  coefficient\n",
      "2         months_tenure       -0.464\n",
      "5     age months_tenure       -0.204\n",
      "7  income months_tenure       -0.184\n",
      "6              income^2       -0.144\n",
      "0                   age       -0.141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Start with simple numeric features for polynomial demo\n",
    "simple_features = df[['age', 'income', 'months_tenure']]\n",
    "\n",
    "print(\"=== POLYNOMIAL FEATURES FOR INTERACTIONS ===\")\n",
    "print(\"Original features:\")\n",
    "print(simple_features.columns.tolist())\n",
    "\n",
    "# Create polynomial features (degree 2 includes interactions)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "X_poly = poly.fit_transform(simple_features)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = poly.get_feature_names_out(simple_features.columns)\n",
    "print(f\"\\nAfter polynomial features (degree=2):\")\n",
    "print(f\"Feature count: {simple_features.shape[1]} → {X_poly.shape[1]}\")\n",
    "print(\"New features created:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    if i >= 3:  # Skip original features\n",
    "        print(f\"• {name}\")\n",
    "\n",
    "# Train model with polynomial features\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n",
    "    X_poly, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler_poly = StandardScaler()\n",
    "X_train_poly_scaled = scaler_poly.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler_poly.transform(X_test_poly)\n",
    "\n",
    "# Use regularization since we have more features now\n",
    "model_poly = LogisticRegression(C=0.1, random_state=42, max_iter=2000)\n",
    "model_poly.fit(X_train_poly_scaled, y_train_poly)\n",
    "\n",
    "auc_poly = roc_auc_score(y_test_poly, model_poly.predict_proba(X_test_poly_scaled)[:, 1])\n",
    "\n",
    "print(f\"\\nPerformance comparison:\")\n",
    "print(f\"• Numeric only: {auc_numeric:.3f}\")\n",
    "print(f\"• With categoricals: {auc_encoded:.3f}\")\n",
    "print(f\"• With polynomials: {auc_poly:.3f}\")\n",
    "\n",
    "# Show most important polynomial features\n",
    "coef_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': model_poly.coef_[0],\n",
    "    'abs_coefficient': np.abs(model_poly.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 5 most important polynomial features:\")\n",
    "print(coef_importance.head()[['feature', 'coefficient']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce5996-042b-4827-be9c-286f6d4c326f",
   "metadata": {},
   "source": [
    "### 5.4 Feature Selection - Finding What Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d6ff688-6a81-4cbc-8225-a8271817d005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SELECTION METHODS ===\n",
      "Statistical selection (top 10 features):\n",
      "• age\n",
      "• income\n",
      "• months_tenure\n",
      "• contract_type_annual\n",
      "• contract_type_monthly\n",
      "• contract_type_two_year\n",
      "• payment_method_bank_transfer\n",
      "• payment_method_credit_card\n",
      "• payment_method_electronic_check\n",
      "• internet_service_no\n",
      "\n",
      "RFE selection (top 10 features):\n",
      "• age\n",
      "• income\n",
      "• months_tenure\n",
      "• contract_type_annual\n",
      "• contract_type_monthly\n",
      "• contract_type_two_year\n",
      "• payment_method_bank_transfer\n",
      "• payment_method_credit_card\n",
      "• payment_method_electronic_check\n",
      "• internet_service_dsl\n",
      "\n",
      "L1 regularization selection (6 features):\n",
      "• age\n",
      "• income\n",
      "• months_tenure\n",
      "• contract_type_monthly\n",
      "• payment_method_electronic_check\n",
      "• internet_service_dsl\n",
      "\n",
      "=== FEATURE SELECTION PERFORMANCE ===\n",
      "Method               Features   AUC     \n",
      "----------------------------------------\n",
      "All features         12         0.770\n",
      "Statistical          10         0.772\n",
      "RFE                  10         0.770\n",
      "L1 regularization    6          0.771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "print(\"=== FEATURE SELECTION METHODS ===\")\n",
    "\n",
    "# Method 1: Statistical selection (SelectKBest)\n",
    "# Select top 10 features based on ANOVA F-test\n",
    "selector_stats = SelectKBest(score_func=f_classif, k=10)\n",
    "X_selected_stats = selector_stats.fit_transform(X_train_enc_scaled, y_train)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features_stats = X_encoded.columns[selector_stats.get_support()]\n",
    "print(\"Statistical selection (top 10 features):\")\n",
    "for feature in selected_features_stats:\n",
    "    print(f\"• {feature}\")\n",
    "\n",
    "# Method 2: Recursive Feature Elimination (RFE)\n",
    "# Use logistic regression to rank features\n",
    "estimator = LogisticRegression(C=1.0, random_state=42, max_iter=2000)\n",
    "selector_rfe = RFE(estimator, n_features_to_select=10, step=1)\n",
    "X_selected_rfe = selector_rfe.fit_transform(X_train_enc_scaled, y_train)\n",
    "\n",
    "selected_features_rfe = X_encoded.columns[selector_rfe.get_support()]\n",
    "print(f\"\\nRFE selection (top 10 features):\")\n",
    "for feature in selected_features_rfe:\n",
    "    print(f\"• {feature}\")\n",
    "\n",
    "# Method 3: L1 regularization (automatic feature selection)\n",
    "model_l1 = LogisticRegression(penalty='l1', C=0.1, solver='liblinear', random_state=42)\n",
    "model_l1.fit(X_train_enc_scaled, y_train)\n",
    "\n",
    "# Features with non-zero coefficients\n",
    "l1_selected = X_encoded.columns[np.abs(model_l1.coef_[0]) > 0.001]\n",
    "print(f\"\\nL1 regularization selection ({len(l1_selected)} features):\")\n",
    "for feature in l1_selected:\n",
    "    print(f\"• {feature}\")\n",
    "\n",
    "# Compare performance of different selection methods\n",
    "selection_results = {}\n",
    "\n",
    "# Original (all features)\n",
    "auc_all = roc_auc_score(y_test, model_encoded.predict_proba(X_test_enc_scaled)[:, 1])\n",
    "selection_results['All features'] = auc_all\n",
    "\n",
    "# Statistical selection\n",
    "model_stats = LogisticRegression(random_state=42)\n",
    "model_stats.fit(X_selected_stats, y_train)\n",
    "X_test_stats = selector_stats.transform(X_test_enc_scaled)\n",
    "auc_stats = roc_auc_score(y_test, model_stats.predict_proba(X_test_stats)[:, 1])\n",
    "selection_results['Statistical (10)'] = auc_stats\n",
    "\n",
    "# RFE selection\n",
    "model_rfe = LogisticRegression(random_state=42)\n",
    "model_rfe.fit(X_selected_rfe, y_train)\n",
    "X_test_rfe = selector_rfe.transform(X_test_enc_scaled)\n",
    "auc_rfe = roc_auc_score(y_test, model_rfe.predict_proba(X_test_rfe)[:, 1])\n",
    "selection_results['RFE (10)'] = auc_rfe\n",
    "\n",
    "# L1 selection\n",
    "auc_l1 = roc_auc_score(y_test, model_l1.predict_proba(X_test_enc_scaled)[:, 1])\n",
    "selection_results['L1 regularization'] = auc_l1\n",
    "\n",
    "print(f\"\\n=== FEATURE SELECTION PERFORMANCE ===\")\n",
    "print(f\"{'Method':<20} {'Features':<10} {'AUC':<8}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'All features':<20} {X_encoded.shape[1]:<10} {auc_all:.3f}\")\n",
    "print(f\"{'Statistical':<20} {10:<10} {auc_stats:.3f}\")\n",
    "print(f\"{'RFE':<20} {10:<10} {auc_rfe:.3f}\")\n",
    "print(f\"{'L1 regularization':<20} {len(l1_selected):<10} {auc_l1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c6c4d-5078-4b67-a2ff-1f5856ef7c28",
   "metadata": {},
   "source": [
    "### 5.5 Real-World Example - Complete Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "188311b7-d983-42fe-86fd-123df5571ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering pipeline:\n",
      "• Numeric features: 3\n",
      "• Categorical features: 3\n",
      "• After one-hot encoding: 12 features\n",
      "• Added income_per_age ratio\n",
      "• Added customer lifecycle features\n",
      "• Final feature count: 15\n",
      "\n",
      "=== FEATURE ENGINEERING IMPACT ===\n",
      "• Baseline (numeric only): 0.669\n",
      "• + Categorical encoding: 0.770\n",
      "• + Complete engineering: 0.769\n",
      "• Total improvement: +0.100\n",
      "\n",
      "Top 8 most important features:\n",
      "1. months_tenure: -0.727 ↓\n",
      "2. contract_type_monthly: 0.399 ↑\n",
      "3. age: -0.340 ↓\n",
      "4. income: -0.261 ↓\n",
      "5. contract_type_annual: -0.247 ↓\n",
      "6. payment_method_electronic_check: 0.231 ↑\n",
      "7. contract_type_two_year: -0.224 ↓\n",
      "8. is_new_customer: 0.170 ↑\n"
     ]
    }
   ],
   "source": [
    "# Create a complete feature engineering function\n",
    "def engineer_features(df, target_col):\n",
    "    \"\"\"Complete feature engineering pipeline\"\"\"\n",
    "    \n",
    "    # Separate target\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Identify feature types\n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"Feature engineering pipeline:\")\n",
    "    print(f\"• Numeric features: {len(numeric_features)}\")\n",
    "    print(f\"• Categorical features: {len(categorical_features)}\")\n",
    "    \n",
    "    # 1. One-hot encode categorical features\n",
    "    if categorical_features:\n",
    "        X_encoded = pd.get_dummies(X, columns=categorical_features, prefix=categorical_features)\n",
    "        print(f\"• After one-hot encoding: {X_encoded.shape[1]} features\")\n",
    "    else:\n",
    "        X_encoded = X.copy()\n",
    "    \n",
    "    # 2. Create some domain-specific features\n",
    "    if 'age' in X_encoded.columns and 'income' in X_encoded.columns:\n",
    "        X_encoded['income_per_age'] = X_encoded['income'] / (X_encoded['age'] + 1)  # Avoid division by zero\n",
    "        print(f\"• Added income_per_age ratio\")\n",
    "    \n",
    "    if 'months_tenure' in X_encoded.columns:\n",
    "        X_encoded['is_new_customer'] = (X_encoded['months_tenure'] < 12).astype(int)\n",
    "        X_encoded['is_loyal_customer'] = (X_encoded['months_tenure'] > 36).astype(int)\n",
    "        print(f\"• Added customer lifecycle features\")\n",
    "    \n",
    "    print(f\"• Final feature count: {X_encoded.shape[1]}\")\n",
    "    \n",
    "    return X_encoded, y\n",
    "\n",
    "# Apply complete feature engineering\n",
    "X_final, y_final = engineer_features(df.copy(), 'churned')\n",
    "\n",
    "# Train final model\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, random_state=42, stratify=y_final\n",
    ")\n",
    "\n",
    "scaler_final = StandardScaler()\n",
    "X_train_final_scaled = scaler_final.fit_transform(X_train_final)\n",
    "X_test_final_scaled = scaler_final.transform(X_test_final)\n",
    "\n",
    "model_final = LogisticRegression(C=0.1, random_state=42, max_iter=2000)\n",
    "model_final.fit(X_train_final_scaled, y_train_final)\n",
    "\n",
    "auc_final = roc_auc_score(y_test_final, model_final.predict_proba(X_test_final_scaled)[:, 1])\n",
    "\n",
    "print(f\"\\n=== FEATURE ENGINEERING IMPACT ===\")\n",
    "print(f\"• Baseline (numeric only): {auc_numeric:.3f}\")\n",
    "print(f\"• + Categorical encoding: {auc_encoded:.3f}\")\n",
    "print(f\"• + Complete engineering: {auc_final:.3f}\")\n",
    "print(f\"• Total improvement: {auc_final - auc_numeric:+.3f}\")\n",
    "\n",
    "# Show most important engineered features\n",
    "feature_importance_final = pd.DataFrame({\n",
    "    'feature': X_final.columns,\n",
    "    'coefficient': model_final.coef_[0],\n",
    "    'abs_coefficient': np.abs(model_final.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 8 most important features:\")\n",
    "for i, (_, row) in enumerate(feature_importance_final.head(8).iterrows()):\n",
    "    direction = \"↑\" if row['coefficient'] > 0 else \"↓\"\n",
    "    print(f\"{i+1}. {row['feature']}: {row['coefficient']:.3f} {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181abf9f-ae1f-465d-b35e-54def155517e",
   "metadata": {},
   "source": [
    "## Section 6 - Production Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b256e-7ef2-4954-a9c0-22c40bf72eab",
   "metadata": {},
   "source": [
    "### 6.1 sklearn Pipeline - Preprocessing + Model in One Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36cf93b3-bc65-4384-8a43-e0934bae27a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRODUCTION PIPELINE SETUP ===\n",
      "Dataset shape: (2000, 7)\n",
      "Numeric features: ['age', 'income', 'months_tenure']\n",
      "Categorical features: ['contract_type', 'payment_method', 'internet_service']\n",
      "\n",
      "Pipeline steps:\n",
      "1. preprocessor: ColumnTransformer\n",
      "2. classifier: LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "# Create production-ready pipeline using previous churn dataset\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Recreate our customer dataset (same as Section 5)\n",
    "np.random.seed(42)\n",
    "n_customers = 2000\n",
    "\n",
    "data = {\n",
    "    'age': np.random.normal(40, 15, n_customers).astype(int),\n",
    "    'income': np.random.normal(50000, 20000, n_customers),\n",
    "    'months_tenure': np.random.uniform(1, 60, n_customers),\n",
    "    'contract_type': np.random.choice(['monthly', 'annual', 'two_year'], n_customers, p=[0.5, 0.3, 0.2]),\n",
    "    'payment_method': np.random.choice(['credit_card', 'bank_transfer', 'electronic_check'], n_customers, p=[0.4, 0.3, 0.3]),\n",
    "    'internet_service': np.random.choice(['dsl', 'fiber', 'no'], n_customers, p=[0.4, 0.4, 0.2])\n",
    "}\n",
    "\n",
    "# Create churn target\n",
    "churn_probability = (\n",
    "    -0.02 * data['age'] +\n",
    "    -0.00001 * data['income'] +\n",
    "    -0.05 * data['months_tenure'] +\n",
    "    (np.array(data['contract_type']) == 'monthly') * 1.5 +\n",
    "    (np.array(data['payment_method']) == 'electronic_check') * 0.8 +\n",
    "    2.0\n",
    ")\n",
    "churn_prob = 1 / (1 + np.exp(-churn_probability))\n",
    "data['churned'] = np.random.binomial(1, churn_prob)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"=== PRODUCTION PIPELINE SETUP ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Define feature types for pipeline\n",
    "numeric_features = ['age', 'income', 'months_tenure']\n",
    "categorical_features = ['contract_type', 'payment_method', 'internet_service']\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing steps\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "    ('scaler', StandardScaler())                     # Scale features\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encode\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Create complete pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(C=0.1, random_state=42, max_iter=2000))\n",
    "])\n",
    "\n",
    "print(f\"\\nPipeline steps:\")\n",
    "for i, (name, step) in enumerate(pipeline.steps):\n",
    "    print(f\"{i+1}. {name}: {type(step).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a233ae1-bbaa-48a4-ac20-71f8a71f7e3e",
   "metadata": {},
   "source": [
    "### 6.2 Training and Evaluating the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85a20771-57c8-437d-9edd-ffa979b3de38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING PRODUCTION PIPELINE ===\n",
      "Pipeline Performance:\n",
      "• Accuracy: 0.718\n",
      "• ROC-AUC: 0.770\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Retained       0.69      0.70      0.70       186\n",
      "     Churned       0.74      0.73      0.73       214\n",
      "\n",
      "    accuracy                           0.72       400\n",
      "   macro avg       0.72      0.72      0.72       400\n",
      "weighted avg       0.72      0.72      0.72       400\n",
      "\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "• months_tenure: -0.857 (decreases churn probability)\n",
      "• contract_type_monthly: 0.823 (increases churn probability)\n",
      "• payment_method_electronic_check: 0.437 (increases churn probability)\n",
      "• contract_type_two_year: -0.430 (decreases churn probability)\n",
      "• contract_type_annual: -0.413 (decreases churn probability)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop('churned', axis=1)\n",
    "y = df['churned']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"=== TRAINING PRODUCTION PIPELINE ===\")\n",
    "\n",
    "# Train pipeline (handles all preprocessing automatically)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(f\"Pipeline Performance:\")\n",
    "print(f\"• Accuracy: {accuracy:.3f}\")\n",
    "print(f\"• ROC-AUC: {auc:.3f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Retained', 'Churned']))\n",
    "\n",
    "# Show what pipeline learned\n",
    "feature_names = (numeric_features + \n",
    "                list(pipeline.named_steps['preprocessor']\n",
    "                    .named_transformers_['cat']\n",
    "                    .named_steps['onehot']\n",
    "                    .get_feature_names_out(categorical_features)))\n",
    "\n",
    "coefficients = pipeline.named_steps['classifier'].coef_[0]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "for _, row in feature_importance.head().iterrows():\n",
    "    direction = \"increases\" if row['coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"• {row['feature']}: {row['coefficient']:.3f} ({direction} churn probability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872760e-b0b7-48e8-954a-921d3f496517",
   "metadata": {},
   "source": [
    "### 6.3 Handling New Data - Edge Cases and Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5d08f24-5ab2-4020-9274-3fa41f1f981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING PIPELINE ROBUSTNESS ===\n",
      "Edge cases to test:\n",
      "    age    income  months_tenure     contract_type      payment_method  \\\n",
      "0  25.0   30000.0            1.0           monthly         credit_card   \n",
      "1   NaN   80000.0           24.0            annual  new_payment_method   \n",
      "2  70.0       NaN            NaN  unknown_contract       bank_transfer   \n",
      "3  18.0  200000.0            0.5          two_year    electronic_check   \n",
      "4  45.0   45000.0           48.0           monthly                 NaN   \n",
      "\n",
      "  internet_service  \n",
      "0            fiber  \n",
      "1              dsl  \n",
      "2        satellite  \n",
      "3               no  \n",
      "4            fiber  \n",
      "\n",
      "✅ Pipeline successfully handled all edge cases!\n",
      "\n",
      "Predictions:\n",
      "Case   Prediction   Churn Prob  Confidence\n",
      "---------------------------------------------\n",
      "1      Will Churn   0.929       0.929\n",
      "2      Will Stay    0.385       0.615\n",
      "3      Will Stay    0.284       0.716\n",
      "4      Will Churn   0.526       0.526\n",
      "5      Will Stay    0.453       0.547\n",
      "\n",
      "🔍 HOW PIPELINE HANDLES EDGE CASES:\n",
      "• Missing numeric values → Filled with median\n",
      "• Missing categorical values → Filled with 'missing' category\n",
      "• Unknown categories → Ignored (all zeros in one-hot encoding)\n",
      "• Extreme values → Scaled with same scaler from training\n"
     ]
    }
   ],
   "source": [
    "# Create challenging test cases that could break a naive model\n",
    "edge_cases = pd.DataFrame({\n",
    "    'age': [25, np.nan, 70, 18, 45],  # Missing value, extreme values\n",
    "    'income': [30000, 80000, np.nan, 200000, 45000],  # Missing income, very high income\n",
    "    'months_tenure': [1, 24, np.nan, 0.5, 48],  # Missing tenure, very short tenure\n",
    "    'contract_type': ['monthly', 'annual', 'unknown_contract', 'two_year', 'monthly'],  # Unknown category\n",
    "    'payment_method': ['credit_card', 'new_payment_method', 'bank_transfer', 'electronic_check', np.nan],  # Unknown method, missing\n",
    "    'internet_service': ['fiber', 'dsl', 'satellite', 'no', 'fiber']  # Unknown service type\n",
    "})\n",
    "\n",
    "print(\"=== TESTING PIPELINE ROBUSTNESS ===\")\n",
    "print(\"Edge cases to test:\")\n",
    "print(edge_cases)\n",
    "\n",
    "# Pipeline handles all edge cases automatically!\n",
    "try:\n",
    "    predictions = pipeline.predict(edge_cases)\n",
    "    probabilities = pipeline.predict_proba(edge_cases)\n",
    "    \n",
    "    print(f\"\\n✅ Pipeline successfully handled all edge cases!\")\n",
    "    \n",
    "    print(f\"\\nPredictions:\")\n",
    "    print(f\"{'Case':<6} {'Prediction':<12} {'Churn Prob':<11} {'Confidence'}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for i in range(len(edge_cases)):\n",
    "        pred_label = 'Will Churn' if predictions[i] == 1 else 'Will Stay'\n",
    "        churn_prob = probabilities[i, 1]\n",
    "        confidence = max(probabilities[i])\n",
    "        \n",
    "        print(f\"{i+1:<6} {pred_label:<12} {churn_prob:<11.3f} {confidence:.3f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Pipeline failed: {e}\")\n",
    "\n",
    "# Show how pipeline handles missing/unknown values\n",
    "print(f\"\\n🔍 HOW PIPELINE HANDLES EDGE CASES:\")\n",
    "print(f\"• Missing numeric values → Filled with median\")\n",
    "print(f\"• Missing categorical values → Filled with 'missing' category\")\n",
    "print(f\"• Unknown categories → Ignored (all zeros in one-hot encoding)\")\n",
    "print(f\"• Extreme values → Scaled with same scaler from training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df37c75-d9a9-455b-b479-12215dff732c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
