{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36aa54b-2f2a-4c85-9d79-c7ac5a6ffd76",
   "metadata": {},
   "source": [
    "# Logistic Regression Primer\n",
    "\n",
    "This is a practical primer for logistic regression implementation\n",
    "\n",
    "**Sections:**\n",
    "- Section 1: Core Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6c51e-d584-405c-9cad-613b53409e71",
   "metadata": {},
   "source": [
    "## 1. Core Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead7f07f-266a-494e-aed8-c45774e02511",
   "metadata": {},
   "source": [
    "### 1.1 Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e491da-15dc-4641-9603-f0ba209b36d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "Features: 30\n",
      "Samples: 569\n",
      "Target classes: ['malignant' 'benign']\n",
      "Class distribution: [212 357]\n",
      "Class balance: 62.7% malignant\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "\n",
    "# Load a binary classification dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(\"Dataset info:\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]}\")\n",
    "print(f\"Target classes: {data.target_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Class balance: {np.bincount(y)[1]/len(y):.1%} malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a03c4c2-faf0-4ebd-b5bd-8079355d2345",
   "metadata": {},
   "source": [
    "### 1.2 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074d3169-670e-4af9-a444-934e5fccdea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split:\n",
      "Training samples: 455\n",
      "Test samples: 114\n",
      "\n",
      "Model trained successfully!\n",
      "Model converged in 2227 iterations\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# stratify ensures that the training and testing sets have the same proportion of classes as the original dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "model = LogisticRegression(random_state=42, max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nModel trained successfully!\")\n",
    "print(f\"Model converged in {model.n_iter_[0]} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4bbb4d-b7f1-4181-a8bb-6efb9a1f497e",
   "metadata": {},
   "source": [
    "### 1.3 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64cd603f-353e-4ea6-9dff-2ec94dc9ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction types:\n",
      "Class predictions shape: (114,)\n",
      "Probability predictions shape (114, 2)\n",
      "\n",
      "First 10 predictions\n",
      "Actual | Predicted | Prob(Benign) | Prob(Malignant)\n",
      "--------------------------------------------------\n",
      "Malignant | Malignant |       0.000 |         1.000\n",
      "Benign   | Benign    |       1.000 |         0.000\n",
      "Malignant | Malignant |       0.050 |         0.950\n",
      "Benign   | Benign    |       0.604 |         0.396\n",
      "Malignant | Malignant |       0.000 |         1.000\n",
      "Benign   | Benign    |       0.983 |         0.017\n",
      "Benign   | Benign    |       1.000 |         0.000\n",
      "Malignant | Malignant |       0.000 |         1.000\n",
      "Malignant | Malignant |       0.000 |         1.000\n",
      "Malignant | Malignant |       0.000 |         1.000\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "print(\"Prediction types:\")\n",
    "print(f\"Class predictions shape: {y_pred.shape}\")\n",
    "print(f\"Probability predictions shape {y_prob.shape}\")\n",
    "\n",
    "# Show the first 10 predictions\n",
    "print(\"\\nFirst 10 predictions\")\n",
    "print(f\"Actual | Predicted | Prob(Benign) | Prob(Malignant)\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(10):\n",
    "    actual = \"Benign\" if y_test[i] == 1 else \"Malignant\"\n",
    "    predicted = \"Benign\" if y_pred[i] == 1 else \"Malignant\"\n",
    "    prob_benign = y_prob[i][1]\n",
    "    prob_malignant = y_prob[i][0]\n",
    "    print(f\"{actual:8} | {predicted:9} | {prob_benign:11.3f} | {prob_malignant:13.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c2d44-0f8f-4776-8725-14fb27f54e2e",
   "metadata": {},
   "source": [
    "### 1.4 Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c52dca57-1cdf-41c8-bd57-4942eda5e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Accuracy: 0.965 (96.5)\n",
      "\n",
      "Detailed Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.97      0.93      0.95        42\n",
      "      benign       0.96      0.99      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "worst concavity: -1.316 decreases malignant probability\n",
      "texture error: 1.092 increases malignant probability\n",
      "mean radius: 0.806 increases malignant probability\n",
      "worst symmetry: -0.782 decreases malignant probability\n",
      "worst compactness: -0.754 decreases malignant probability\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.3f} ({accuracy*100:.1f})\")\n",
    "\n",
    "print(\"\\nDetailed Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "\n",
    "feature_names = data.feature_names\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients\n",
    "})\n",
    "feature_importance['abs_coef'] = abs(feature_importance['coefficient'])\n",
    "top_features = feature_importance.nlargest(5, 'abs_coef')\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for _, row in top_features.iterrows():\n",
    "    direction = \"increases\" if row['coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"{row['feature']}: {row['coefficient']:.3f} {direction} malignant probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5900e-b3dc-4ad4-bfd9-f37cdc179a77",
   "metadata": {},
   "source": [
    "## 2: Data Preprocessing & Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c545a9d-4726-43ea-a0cb-7bc99b44800a",
   "metadata": {},
   "source": [
    "### 2.1 Why Scaling Matters - Demonstrating the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54cecd28-01f6-4d8a-99db-5bdfa94a5ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Scale Analysis:\n",
      "Features with HUGE scale differences:\n",
      "              feature     min     max    range\n",
      "12            proline  278.00  1680.0  1402.00\n",
      "4           magnesium   70.00   162.0    92.00\n",
      "3   alcalinity_of_ash   10.60    30.0    19.40\n",
      "9     color_intensity    1.28    13.0    11.72\n",
      "1          malic_acid    0.74     5.8     5.06\n",
      "\n",
      "Features with small scales:\n",
      "                         feature   min   max  range\n",
      "7           nonflavanoid_phenols  0.13  0.66   0.53\n",
      "10                           hue  0.48  1.71   1.23\n",
      "2                            ash  1.36  3.23   1.87\n",
      "11  od280/od315_of_diluted_wines  1.27  4.00   2.73\n",
      "5                  total_phenols  0.98  3.88   2.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # To hide convergence warning\n",
    "\n",
    "data = load_wine()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "y_binary = (y==0).astype(int)\n",
    "\n",
    "feature_scales = pd.DataFrame({\n",
    "    'feature': data.feature_names,\n",
    "    'min': X.min(axis=0),\n",
    "    'max': X.max(axis=0),\n",
    "    'range': X.max(axis=0) - X.min(axis=0)\n",
    "}).round(2)\n",
    "\n",
    "print(\"Feature Scale Analysis:\")\n",
    "print(\"Features with HUGE scale differences:\")\n",
    "print(feature_scales.nlargest(5, 'range')[['feature', 'min', 'max', 'range']])\n",
    "print(\"\\nFeatures with small scales:\")\n",
    "print(feature_scales.nsmallest(5, 'range')[['feature', 'min', 'max', 'range']])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4c739-3230-4e5e-9fe5-83d8e72a3622",
   "metadata": {},
   "source": [
    "### 2.2 Training Without Scaling - See the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "161fa0a5-34d4-4905-988a-a3be5e2119d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WITHOUT scaling:\n",
      "Converged: False\n",
      "Iterations used: 100/100\n",
      "Accuracy without scaling: 1.000\n",
      "\n",
      "Coefficient range: -0.570768 to 1.222616\n",
      "Problem: Coefficients vary wildly due to scale differences!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Training WITHOUT scaling:\")\n",
    "model_unscaled = LogisticRegression(random_state=42, max_iter=100)\n",
    "model_unscaled.fit(X_train, y_train)\n",
    "\n",
    "# Checking for convergence\n",
    "print(f\"Converged: {model_unscaled.n_iter_[0] < 100}\")\n",
    "print(f\"Iterations used: {model_unscaled.n_iter_[0]}/100\")\n",
    "\n",
    "# Checking performance\n",
    "y_pred_unscaled = model_unscaled.predict(X_test)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "print(f\"Accuracy without scaling: {accuracy_unscaled:.3f}\")\n",
    "\n",
    "# Checking the coefficients\n",
    "coef_unscaled = model_unscaled.coef_[0]\n",
    "print(f\"\\nCoefficient range: {coef_unscaled.min():.6f} to {coef_unscaled.max():.6f}\")\n",
    "print(\"Problem: Coefficients vary wildly due to scale differences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195f813-b87a-4654-bcbe-02577783d544",
   "metadata": {},
   "source": [
    "### 2.3 Training With StandardScaler - The Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9681c349-ad23-44e0-a04e-a6680e7e9da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WITH StandardScaler:\n",
      "After scaling - feature statistics:\n",
      "Mean: [4.35957999e-15 1.10475009e-15 2.02576610e-15]\n",
      "Std:  [1. 1. 1.]\n",
      "\n",
      "Converged: True\n",
      "Iterations used: 12/100\n",
      "Accuracy with scaling: 0.972\n",
      "\n",
      "Coefficient range: -1.096 to 1.784\n",
      "Much better! Coefficients are now comparable in magnitude\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Training WITH StandardScaler:\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform test data with same scaler and avoiding data leakage\n",
    "\n",
    "print(\"After scaling - feature statistics:\")\n",
    "print(f\"Mean: {X_train_scaled.mean(axis=0)[:3]}\")  # Should be ~0\n",
    "print(f\"Std:  {X_train_scaled.std(axis=0)[:3]}\")   # Should be ~1\n",
    "\n",
    "# Train model on scaled data\n",
    "model_scaled = LogisticRegression(random_state=42, max_iter=100)  # Same low max_iter\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nConverged: {model_scaled.n_iter_[0] < 100}\")\n",
    "print(f\"Iterations used: {model_scaled.n_iter_[0]}/100\")\n",
    "\n",
    "# Performance\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f\"Accuracy with scaling: {accuracy_scaled:.3f}\")\n",
    "\n",
    "# Coefficients are now comparable\n",
    "coef_scaled = model_scaled.coef_[0]\n",
    "print(f\"\\nCoefficient range: {coef_scaled.min():.3f} to {coef_scaled.max():.3f}\")\n",
    "print(\"Much better! Coefficients are now comparable in magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c948d62-9e83-441e-99cf-744d81dfc04e",
   "metadata": {},
   "source": [
    "### 2.4 Impact Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e08aea6-aa5f-4255-8e37-16aa48a660a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCALING IMPACT COMPARISON ===\n",
      "Metric               Without Scaling With Scaling    Improvement\n",
      "-----------------------------------------------------------------\n",
      "Convergence          False           True            ‚úì\n",
      "Iterations           100             12              +88\n",
      "Accuracy             1.000           0.972           -0.028\n",
      "Coef Range           1.793           2.880           +1.087\n"
     ]
    }
   ],
   "source": [
    "# Direct comparison\n",
    "print(\"=== SCALING IMPACT COMPARISON ===\")\n",
    "print(f\"{'Metric':<20} {'Without Scaling':<15} {'With Scaling':<15} {'Improvement'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "metrics = {\n",
    "    'Convergence': [model_unscaled.n_iter_[0] < 100, model_scaled.n_iter_[0] < 100],\n",
    "    'Iterations': [model_unscaled.n_iter_[0], model_scaled.n_iter_[0]],\n",
    "    'Accuracy': [accuracy_unscaled, accuracy_scaled],\n",
    "    'Coef Range': [coef_unscaled.max() - coef_unscaled.min(), \n",
    "                   coef_scaled.max() - coef_scaled.min()]\n",
    "}\n",
    "\n",
    "for metric, values in metrics.items():\n",
    "    if metric == 'Convergence':\n",
    "        print(f\"{metric:<20} {str(values[0]):<15} {str(values[1]):<15} {'‚úì' if values[1] else '‚úó'}\")\n",
    "    elif metric == 'Iterations':\n",
    "        improvement = f\"{values[0] - values[1]:+d}\"\n",
    "        print(f\"{metric:<20} {values[0]:<15} {values[1]:<15} {improvement}\")\n",
    "    else:\n",
    "        improvement = f\"{values[1] - values[0]:+.3f}\"\n",
    "        print(f\"{metric:<20} {values[0]:<15.3f} {values[1]:<15.3f} {improvement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de472f7a-0380-47b5-a63e-f073d3905a4a",
   "metadata": {},
   "source": [
    "## 3. Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7056942-f461-429b-9555-e73afb8e5b3b",
   "metadata": {},
   "source": [
    "### 3.1 Detecting Imbalance - The Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f418334e-a315-4881-b101-1d7d09ba4cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Imbalance Analysis:\n",
      "Class 0: 9,456 samples (94.6%)\n",
      "Class 1: 544 samples (5.4%)\n",
      "\n",
      "Dataset: 10,000 transactions\n",
      "Fraud rate: 5.4% (highly imbalanced!)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate imbalanced data: 5% fraud, 95% normal transactions\n",
    "X, y = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=3,\n",
    "    weights=[0.95, 0.05],  # 95% class 0, 5% class 1\n",
    "    flip_y=0.01,           # Add some noise\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Class Imbalance Analysis:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for class_label, count in zip(unique, counts):\n",
    "    percentage = count / len(y) * 100\n",
    "    print(f\"Class {class_label}: {count:,} samples ({percentage:.1f}%)\")\n",
    "\n",
    "class_names = ['Normal', 'Fraud']\n",
    "print(f\"\\nDataset: {X.shape[0]:,} transactions\")\n",
    "print(f\"Fraud rate: {np.mean(y):.1%} (highly imbalanced!)\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2bc03-535d-43a1-a5b8-584f0f7aa9c7",
   "metadata": {},
   "source": [
    "### 3.2 Why Accuracy Fails - Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad961e03-0c83-48fc-b99d-bdb6ff50ebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASIC MODEL (No Class Balancing) ===\n",
      "Accuracy: 0.949 (94.9%)\n",
      "\n",
      "Detailed Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.95      1.00      0.97      1891\n",
      "       Fraud       0.73      0.10      0.18       109\n",
      "\n",
      "    accuracy                           0.95      2000\n",
      "   macro avg       0.84      0.55      0.58      2000\n",
      "weighted avg       0.94      0.95      0.93      2000\n",
      "\n",
      "\n",
      "THE PROBLEM:\n",
      "Fraud cases detected: 11/109\n",
      "Fraud detection rate: 10.1%\n",
      "Model might just predict 'Normal' for everything and still get 95% accuracy!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features (always needed for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train basic model (no class balancing)\n",
    "model_basic = LogisticRegression(random_state=42)\n",
    "model_basic.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate with accuracy only\n",
    "y_pred_basic = model_basic.predict(X_test_scaled)\n",
    "accuracy_basic = accuracy_score(y_test, y_pred_basic)\n",
    "\n",
    "print(\"=== BASIC MODEL (No Class Balancing) ===\")\n",
    "print(f\"Accuracy: {accuracy_basic:.3f} ({accuracy_basic*100:.1f}%)\")\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(classification_report(y_test, y_pred_basic, target_names=class_names))\n",
    "\n",
    "# Show the problem with accuracy\n",
    "fraud_detected = np.sum((y_test == 1) & (y_pred_basic == 1))\n",
    "total_fraud = np.sum(y_test == 1)\n",
    "print(f\"\\nTHE PROBLEM:\")\n",
    "print(f\"Fraud cases detected: {fraud_detected}/{total_fraud}\")\n",
    "print(f\"Fraud detection rate: {fraud_detected/total_fraud:.1%}\")\n",
    "print(f\"Model might just predict 'Normal' for everything and still get 95% accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790906f-25b5-4885-99ba-0b5d02500f55",
   "metadata": {},
   "source": [
    "### 3.3 Better Metrics - Understanding What Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "180ce35a-d56f-4069-a18c-92a65a4ff3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNDERSTANDING METRICS FOR IMBALANCED DATA ===\n",
      "Accuracy: 0.949\n",
      "Precision: 0.733\n",
      "Recall: 0.101\n",
      "F1-Score: 0.177\n",
      "ROC-AUC: 0.810\n",
      "\n",
      "üìö METRIC EXPLANATIONS:\n",
      "‚Ä¢ Accuracy: Overall correct predictions (misleading with imbalance)\n",
      "‚Ä¢ Precision: Of predicted fraud, how many were actually fraud?\n",
      "‚Ä¢ Recall: Of actual fraud, how many did we catch?\n",
      "‚Ä¢ F1-Score: Harmonic mean of precision and recall\n",
      "‚Ä¢ ROC-AUC: How well can model distinguish between classes?\n",
      "\n",
      "üìä CONFUSION MATRIX BREAKDOWN:\n",
      "True Negatives (Normal correctly): 1887\n",
      "False Positives (Normal as Fraud): 4\n",
      "False Negatives (Fraud as Normal): 98 ‚ö†Ô∏è BAD!\n",
      "True Positives (Fraud correctly): 11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Calculate better metrics for imbalanced data\n",
    "y_prob_basic = model_basic.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "metrics_basic = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_basic),\n",
    "    'Precision': precision_score(y_test, y_pred_basic),\n",
    "    'Recall': recall_score(y_test, y_pred_basic),\n",
    "    'F1-Score': f1_score(y_test, y_pred_basic),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_prob_basic)\n",
    "}\n",
    "\n",
    "print(\"=== UNDERSTANDING METRICS FOR IMBALANCED DATA ===\")\n",
    "for metric, value in metrics_basic.items():\n",
    "    print(f\"{metric}: {value:.3f}\")\n",
    "\n",
    "print(f\"\\nüìö METRIC EXPLANATIONS:\")\n",
    "print(f\"‚Ä¢ Accuracy: Overall correct predictions (misleading with imbalance)\")\n",
    "print(f\"‚Ä¢ Precision: Of predicted fraud, how many were actually fraud?\")\n",
    "print(f\"‚Ä¢ Recall: Of actual fraud, how many did we catch?\")\n",
    "print(f\"‚Ä¢ F1-Score: Harmonic mean of precision and recall\")\n",
    "print(f\"‚Ä¢ ROC-AUC: How well can model distinguish between classes?\")\n",
    "\n",
    "# Show confusion matrix breakdown\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_basic)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nüìä CONFUSION MATRIX BREAKDOWN:\")\n",
    "print(f\"True Negatives (Normal correctly): {tn}\")\n",
    "print(f\"False Positives (Normal as Fraud): {fp}\")  \n",
    "print(f\"False Negatives (Fraud as Normal): {fn} ‚ö†Ô∏è BAD!\")\n",
    "print(f\"True Positives (Fraud correctly): {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b76af-2403-42df-b00d-1fa5d98b9f6e",
   "metadata": {},
   "source": [
    "### 3.4 Class Weights - The Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93e89e55-ac4a-4af8-956a-adf834bcd204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARISON: BASIC vs BALANCED ===\n",
      "Metric       Basic    Balanced   Change\n",
      "----------------------------------------\n",
      "Accuracy     0.949    0.761      -0.188\n",
      "Precision    0.733    0.157      -0.577\n",
      "Recall       0.101    0.771      +0.670\n",
      "F1-Score     0.177    0.260      +0.083\n",
      "ROC-AUC      0.810    0.830      +0.020\n",
      "\n",
      "‚öñÔ∏è CLASS WEIGHTS EXPLANATION:\n",
      "Normal class weight: 0.529\n",
      "Fraud class weight: 9.195\n",
      "Fraud gets 17.4x more weight in training\n"
     ]
    }
   ],
   "source": [
    "# Train model with balanced class weights\n",
    "model_balanced = LogisticRegression(\n",
    "    class_weight='balanced',  # Automatically balances classes\n",
    "    random_state=42\n",
    ")\n",
    "model_balanced.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compare results\n",
    "y_pred_balanced = model_balanced.predict(X_test_scaled)\n",
    "y_prob_balanced = model_balanced.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "metrics_balanced = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_balanced),\n",
    "    'Precision': precision_score(y_test, y_pred_balanced),\n",
    "    'Recall': recall_score(y_test, y_pred_balanced),\n",
    "    'F1-Score': f1_score(y_test, y_pred_balanced),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_prob_balanced)\n",
    "}\n",
    "\n",
    "print(\"=== COMPARISON: BASIC vs BALANCED ===\")\n",
    "print(f\"{'Metric':<12} {'Basic':<8} {'Balanced':<10} {'Change'}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for metric in metrics_basic.keys():\n",
    "    basic_val = metrics_basic[metric]\n",
    "    balanced_val = metrics_balanced[metric]\n",
    "    change = balanced_val - basic_val\n",
    "    change_str = f\"{change:+.3f}\"\n",
    "    print(f\"{metric:<12} {basic_val:<8.3f} {balanced_val:<10.3f} {change_str}\")\n",
    "\n",
    "# Show what class_weight='balanced' does\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "print(f\"\\n‚öñÔ∏è CLASS WEIGHTS EXPLANATION:\")\n",
    "print(f\"Normal class weight: {class_weights[0]:.3f}\")\n",
    "print(f\"Fraud class weight: {class_weights[1]:.3f}\")\n",
    "print(f\"Fraud gets {class_weights[1]/class_weights[0]:.1f}x more weight in training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194dbab9-7e5f-45f4-84a5-63fb343174df",
   "metadata": {},
   "source": [
    "### 3.5 Custom Threshold Tuning - Business Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0725d328-a648-4497-be27-6a3cb56851b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== THRESHOLD OPTIMIZATION ===\n",
      "Default threshold: 0.5\n",
      "Optimal F1 threshold: 0.826\n",
      "\n",
      "üéØ BUSINESS SCENARIO COMPARISON:\n",
      "Scenario                  Threshold   Precision  Recall   F1\n",
      "-----------------------------------------------------------------\n",
      "Conservative (High Precision) 0.800       0.327      0.339    0.333\n",
      "Aggressive (High Recall)  0.300       0.095      0.890    0.172\n",
      "Balanced F1               0.826       0.391      0.312    0.347\n",
      "\n",
      "üí° BUSINESS INTERPRETATION:\n",
      "‚Ä¢ Conservative: Minimize false alarms, might miss some fraud\n",
      "‚Ä¢ Aggressive: Catch most fraud, but more false alarms\n",
      "‚Ä¢ Balanced: Best overall trade-off between precision and recall\n"
     ]
    }
   ],
   "source": [
    "# Sometimes you need custom thresholds for business needs\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find optimal threshold for different business objectives\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_prob_balanced)\n",
    "\n",
    "# Calculate F1 scores for all thresholds\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "optimal_threshold_f1 = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(\"=== THRESHOLD OPTIMIZATION ===\")\n",
    "print(f\"Default threshold: 0.5\")\n",
    "print(f\"Optimal F1 threshold: {optimal_threshold_f1:.3f}\")\n",
    "\n",
    "# Test different business scenarios\n",
    "scenarios = {\n",
    "    'Conservative (High Precision)': 0.8,  # Only flag if very confident\n",
    "    'Aggressive (High Recall)': 0.3,      # Flag more cases to catch fraud\n",
    "    'Balanced F1': optimal_threshold_f1    # Optimal F1 score\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ BUSINESS SCENARIO COMPARISON:\")\n",
    "print(f\"{'Scenario':<25} {'Threshold':<11} {'Precision':<10} {'Recall':<8} {'F1'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for scenario, threshold in scenarios.items():\n",
    "    y_pred_custom = (y_prob_balanced >= threshold).astype(int)\n",
    "    prec = precision_score(y_test, y_pred_custom)\n",
    "    rec = recall_score(y_test, y_pred_custom)\n",
    "    f1 = f1_score(y_test, y_pred_custom)\n",
    "    \n",
    "    print(f\"{scenario:<25} {threshold:<11.3f} {prec:<10.3f} {rec:<8.3f} {f1:.3f}\")\n",
    "\n",
    "print(f\"\\nüí° BUSINESS INTERPRETATION:\")\n",
    "print(f\"‚Ä¢ Conservative: Minimize false alarms, might miss some fraud\")\n",
    "print(f\"‚Ä¢ Aggressive: Catch most fraud, but more false alarms\")\n",
    "print(f\"‚Ä¢ Balanced: Best overall trade-off between precision and recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade142a-e637-4aa1-b6d9-28c7ff806a7b",
   "metadata": {},
   "source": [
    "## 4. Regularization & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de913c1a-af43-4357-9a3c-dd773649bd2a",
   "metadata": {},
   "source": [
    "### 4.1 L1 vs L2 Regularization - Understanding the Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4789aec-0aa9-4dd0-8cd3-761c1ca48dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Dimensional Dataset:\n",
      "Total features: 50\n",
      "Informative features: 10\n",
      "Irrelevant features: 40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Dataset with many features (some irrelevant)\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=50,        # 50 features total\n",
    "    n_informative=10,     # Only 10 are actually useful\n",
    "    n_redundant=5,        # 5 are redundant\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"High-Dimensional Dataset:\")\n",
    "print(f\"Total features: {X.shape[1]}\")\n",
    "print(f\"Informative features: 10\")\n",
    "print(f\"Irrelevant features: {50 - 10}\")\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa2ad38-5128-4c0a-b2f3-c033b35f84c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REGULARIZATION COMPARISON ===\n",
      "Model              Train Acc  Test Acc  ROC-AUC  Non-Zero Coef\n",
      "------------------------------------------------------------\n",
      "No Regularization  0.966      0.950     0.995    50\n",
      "L1 (Lasso)         0.963      0.965     0.997    9\n",
      "L2 (Ridge)         0.965      0.965     0.997    50\n",
      "\n",
      "üéØ OVERFITTING ANALYSIS:\n",
      "No Regularization: 0.016 (lower is better)\n",
      "L1 (Lasso): -0.002 (lower is better)\n",
      "L2 (Ridge): 0.000 (lower is better)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Compare different regularization approaches\n",
    "models = {\n",
    "    'No Regularization': LogisticRegression(C=1e6, random_state=42, max_iter=2000),  # Very high C = almost no regularization\n",
    "    'L1 (Lasso)': LogisticRegression(penalty='l1', C=0.1, solver='liblinear', random_state=42),\n",
    "    'L2 (Ridge)': LogisticRegression(penalty='l2', C=0.1, random_state=42, max_iter=2000)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "print(\"=== REGULARIZATION COMPARISON ===\")\n",
    "print(f\"{'Model':<18} {'Train Acc':<10} {'Test Acc':<9} {'ROC-AUC':<8} {'Non-Zero Coef'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train_scaled))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "    test_auc = roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:, 1])\n",
    "    \n",
    "    # Count non-zero coefficients (feature selection)\n",
    "    non_zero_coef = np.sum(np.abs(model.coef_[0]) > 0.001)\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'auc': test_auc,\n",
    "        'non_zero_coef': non_zero_coef,\n",
    "        'coefficients': model.coef_[0]\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:<18} {train_acc:<10.3f} {test_acc:<9.3f} {test_auc:<8.3f} {non_zero_coef}\")\n",
    "\n",
    "# Show overfitting\n",
    "print(f\"\\nüéØ OVERFITTING ANALYSIS:\")\n",
    "for name, result in results.items():\n",
    "    overfitting = result['train_acc'] - result['test_acc']\n",
    "    print(f\"{name}: {overfitting:.3f} (lower is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff6972-c9be-4b7a-8324-12105e3307ce",
   "metadata": {},
   "source": [
    "### 4.3 Understanding the C Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b6176d9-f841-4558-abe3-6027921eaba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== C PARAMETER IMPACT (L2 Regularization) ===\n",
      "C Value  Train Acc  Test Acc  Overfitting Non-Zero Coef\n",
      "-------------------------------------------------------\n",
      "0.001    0.949      0.950     -0.001      47\n",
      "0.01     0.960      0.960     0.000       50\n",
      "0.1      0.965      0.965     0.000       50\n",
      "1.0      0.963      0.955     0.008       50\n",
      "10.0     0.964      0.950     0.014       50\n",
      "100.0    0.966      0.950     0.016       50\n",
      "\n",
      "üí° C PARAMETER INTERPRETATION:\n",
      "‚Ä¢ High C (100): Less regularization ‚Üí More complex model ‚Üí Potential overfitting\n",
      "‚Ä¢ Low C (0.001): More regularization ‚Üí Simpler model ‚Üí Potential underfitting\n",
      "‚Ä¢ Sweet spot: Usually between 0.1 and 10.0\n",
      "‚Ä¢ Best C from our test: 0.001 (lowest overfitting)\n"
     ]
    }
   ],
   "source": [
    "# Test different C values to understand regularization strength\n",
    "C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "print(\"=== C PARAMETER IMPACT (L2 Regularization) ===\")\n",
    "print(f\"{'C Value':<8} {'Train Acc':<10} {'Test Acc':<9} {'Overfitting':<11} {'Non-Zero Coef'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "c_results = {}\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(penalty='l2', C=C, random_state=42, max_iter=2000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train_scaled))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "    overfitting = train_acc - test_acc\n",
    "    non_zero_coef = np.sum(np.abs(model.coef_[0]) > 0.001)\n",
    "    \n",
    "    c_results[C] = {'train': train_acc, 'test': test_acc, 'overfitting': overfitting}\n",
    "    \n",
    "    print(f\"{C:<8} {train_acc:<10.3f} {test_acc:<9.3f} {overfitting:<11.3f} {non_zero_coef}\")\n",
    "\n",
    "print(f\"\\nüí° C PARAMETER INTERPRETATION:\")\n",
    "print(f\"‚Ä¢ High C (100): Less regularization ‚Üí More complex model ‚Üí Potential overfitting\")\n",
    "print(f\"‚Ä¢ Low C (0.001): More regularization ‚Üí Simpler model ‚Üí Potential underfitting\")\n",
    "print(f\"‚Ä¢ Sweet spot: Usually between 0.1 and 10.0\")\n",
    "\n",
    "# Find best C from our test\n",
    "best_c = min(c_results.keys(), key=lambda x: c_results[x]['overfitting'])\n",
    "print(f\"‚Ä¢ Best C from our test: {best_c} (lowest overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b4cac-4a44-4f33-90dd-85ca9b5eca52",
   "metadata": {},
   "source": [
    "### 4.4 GridSearchCV - Systematic Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "327c8012-1165-4782-b337-50c569437592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRIDSEARCHCV FOR OPTIMAL HYPERPARAMETERS ===\n",
      "Searching through parameter combinations...\n",
      "Best parameters: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best cross-validation AUC: 0.990\n",
      "Test set performance:\n",
      "‚Ä¢ Test AUC: 0.997\n",
      "‚Ä¢ Test Accuracy: 0.965\n",
      "‚Ä¢ Features selected by L1: 9/50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']  # Works with both L1 and L2\n",
    "}\n",
    "\n",
    "print(\"=== GRIDSEARCHCV FOR OPTIMAL HYPERPARAMETERS ===\")\n",
    "print(\"Searching through parameter combinations...\")\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=42, max_iter=2000),\n",
    "    param_grid,\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring='roc_auc',       # Use AUC for imbalanced data\n",
    "    n_jobs=-1,               # Use all CPU cores\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation AUC: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate best model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "test_acc = accuracy_score(y_test, best_model.predict(X_test_scaled))\n",
    "\n",
    "print(f\"Test set performance:\")\n",
    "print(f\"‚Ä¢ Test AUC: {test_auc:.3f}\")\n",
    "print(f\"‚Ä¢ Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# Show feature selection with best model\n",
    "if grid_search.best_params_['penalty'] == 'l1':\n",
    "    selected_features = np.sum(np.abs(best_model.coef_[0]) > 0.001)\n",
    "    print(f\"‚Ä¢ Features selected by L1: {selected_features}/{X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25efffb-652e-48ae-9d27-b51f8ce2f1a7",
   "metadata": {},
   "source": [
    "### 4.5 Cross-Validation - Reliable Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "854443b1-d6d4-4a03-aec8-1eaab576c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CROSS-VALIDATION FOR RELIABLE EVALUATION ===\n",
      "\n",
      "Baseline (no reg):\n",
      "‚Ä¢ CV Accuracy: 0.941 ¬± 0.017\n",
      "‚Ä¢ CV ROC-AUC: 0.986 ¬± 0.008\n",
      "‚Ä¢ CV Precision: 0.939 ¬± 0.028\n",
      "‚Ä¢ CV Recall: 0.945 ¬± 0.010\n",
      "‚Ä¢ Overfitting (Train-CV): 0.010\n",
      "\n",
      "Best from Grid:\n",
      "‚Ä¢ CV Accuracy: 0.963 ¬± 0.017\n",
      "‚Ä¢ CV ROC-AUC: 0.990 ¬± 0.007\n",
      "‚Ä¢ CV Precision: 0.956 ¬± 0.026\n",
      "‚Ä¢ CV Recall: 0.970 ¬± 0.013\n",
      "‚Ä¢ Overfitting (Train-CV): 0.001\n",
      "\n",
      "üìä WHY CROSS-VALIDATION MATTERS:\n",
      "‚Ä¢ Single train/test split can be lucky or unlucky\n",
      "‚Ä¢ CV uses multiple splits for reliable estimates\n",
      "‚Ä¢ Standard deviation shows consistency across folds\n",
      "‚Ä¢ Compare train vs CV scores to detect overfitting\n",
      "\n",
      "üèÜ FINAL MODEL SELECTED:\n",
      "‚Ä¢ Algorithm: LogisticRegression\n",
      "‚Ä¢ Penalty: l1\n",
      "‚Ä¢ C: 0.1\n",
      "‚Ä¢ Expected AUC: 0.990\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Demonstrate proper cross-validation\n",
    "print(\"=== CROSS-VALIDATION FOR RELIABLE EVALUATION ===\")\n",
    "\n",
    "# Compare our best model with baseline\n",
    "models_to_compare = {\n",
    "    'Baseline (no reg)': LogisticRegression(C=1e6, random_state=42, max_iter=2000),\n",
    "    'Best from Grid': grid_search.best_estimator_\n",
    "}\n",
    "\n",
    "cv_scores = {}\n",
    "for name, model in models_to_compare.items():\n",
    "    # Multiple metrics with cross-validation\n",
    "    scores = cross_validate(\n",
    "        model, X_train_scaled, y_train,\n",
    "        cv=5,\n",
    "        scoring=['accuracy', 'roc_auc', 'precision', 'recall'],\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    cv_scores[name] = scores\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"‚Ä¢ CV Accuracy: {scores['test_accuracy'].mean():.3f} ¬± {scores['test_accuracy'].std():.3f}\")\n",
    "    print(f\"‚Ä¢ CV ROC-AUC: {scores['test_roc_auc'].mean():.3f} ¬± {scores['test_roc_auc'].std():.3f}\")\n",
    "    print(f\"‚Ä¢ CV Precision: {scores['test_precision'].mean():.3f} ¬± {scores['test_precision'].std():.3f}\")\n",
    "    print(f\"‚Ä¢ CV Recall: {scores['test_recall'].mean():.3f} ¬± {scores['test_recall'].std():.3f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    train_auc = scores['train_roc_auc'].mean()\n",
    "    test_auc = scores['test_roc_auc'].mean()\n",
    "    overfitting = train_auc - test_auc\n",
    "    print(f\"‚Ä¢ Overfitting (Train-CV): {overfitting:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä WHY CROSS-VALIDATION MATTERS:\")\n",
    "print(f\"‚Ä¢ Single train/test split can be lucky or unlucky\")\n",
    "print(f\"‚Ä¢ CV uses multiple splits for reliable estimates\")\n",
    "print(f\"‚Ä¢ Standard deviation shows consistency across folds\")\n",
    "print(f\"‚Ä¢ Compare train vs CV scores to detect overfitting\")\n",
    "\n",
    "# Final model selection\n",
    "final_model = grid_search.best_estimator_\n",
    "print(f\"\\nüèÜ FINAL MODEL SELECTED:\")\n",
    "print(f\"‚Ä¢ Algorithm: LogisticRegression\")\n",
    "print(f\"‚Ä¢ Penalty: {grid_search.best_params_['penalty']}\")\n",
    "print(f\"‚Ä¢ C: {grid_search.best_params_['C']}\")\n",
    "print(f\"‚Ä¢ Expected AUC: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2300af-d4d6-42b9-a560-a273ea199073",
   "metadata": {},
   "source": [
    "## Section 5 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e0c12-06f1-437f-883b-cec4b5a5b25d",
   "metadata": {},
   "source": [
    "### 5.1 Categorical Encoding - Handling Non-Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73895e91-e3a2-45f0-8fb6-16ac6b9e35e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MIXED DATA TYPES DATASET ===\n",
      "Dataset shape: (2000, 7)\n",
      "Churn rate: 53.5%\n",
      "\n",
      "Data types:\n",
      "age                   int32\n",
      "income              float64\n",
      "months_tenure       float64\n",
      "contract_type        object\n",
      "payment_method       object\n",
      "internet_service     object\n",
      "churned               int32\n",
      "dtype: object\n",
      "\n",
      "Categorical variables:\n",
      "‚Ä¢ contract_type: 3 categories ['annual', 'monthly', 'two_year']\n",
      "‚Ä¢ payment_method: 3 categories ['bank_transfer', 'credit_card', 'electronic_check']\n",
      "‚Ä¢ internet_service: 3 categories ['dsl', 'fiber', 'no']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Customer churn dataset with mixed feature types\n",
    "np.random.seed(42)\n",
    "n_customers = 2000\n",
    "\n",
    "# Create realistic customer data\n",
    "data = {\n",
    "    'age': np.random.normal(40, 15, n_customers).astype(int),\n",
    "    'income': np.random.normal(50000, 20000, n_customers),\n",
    "    'months_tenure': np.random.uniform(1, 60, n_customers),\n",
    "    'contract_type': np.random.choice(['monthly', 'annual', 'two_year'], n_customers, p=[0.5, 0.3, 0.2]),\n",
    "    'payment_method': np.random.choice(['credit_card', 'bank_transfer', 'electronic_check'], n_customers, p=[0.4, 0.3, 0.3]),\n",
    "    'internet_service': np.random.choice(['dsl', 'fiber', 'no'], n_customers, p=[0.4, 0.4, 0.2])\n",
    "}\n",
    "\n",
    "# Create logical churn patterns\n",
    "churn_probability = (\n",
    "    -0.02 * data['age'] +                    # Younger customers churn more\n",
    "    -0.00001 * data['income'] +              # Higher income customers churn less\n",
    "    -0.05 * data['months_tenure'] +          # Longer tenure = less churn\n",
    "    (np.array(data['contract_type']) == 'monthly') * 1.5 +  # Monthly contracts churn more\n",
    "    (np.array(data['payment_method']) == 'electronic_check') * 0.8 +  # Electronic check risky\n",
    "    2.0  # Base churn rate\n",
    ")\n",
    "\n",
    "# Convert to probability and binary outcome\n",
    "churn_prob = 1 / (1 + np.exp(-churn_probability))\n",
    "data['churned'] = np.random.binomial(1, churn_prob)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"=== MIXED DATA TYPES DATASET ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Churn rate: {df['churned'].mean():.1%}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nCategorical variables:\")\n",
    "categorical_cols = ['contract_type', 'payment_method', 'internet_service']\n",
    "for col in categorical_cols:\n",
    "    print(f\"‚Ä¢ {col}: {df[col].nunique()} categories {list(df[col].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb5061-542b-48ee-bdd3-a5e0a27101b5",
   "metadata": {},
   "source": [
    "### 5.2 One-Hot Encoding with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "659efb6c-e678-4b84-b331-469d82b7137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE ONE-HOT ENCODING ===\n",
      "AUC with numeric features only: 0.669\n",
      "\n",
      "=== AFTER ONE-HOT ENCODING ===\n",
      "Original features: 7\n",
      "After encoding: 13\n",
      "New categorical features created:\n",
      "‚Ä¢ contract_type ‚Üí ['contract_type_annual', 'contract_type_monthly', 'contract_type_two_year']\n",
      "‚Ä¢ payment_method ‚Üí ['payment_method_bank_transfer', 'payment_method_credit_card', 'payment_method_electronic_check']\n",
      "‚Ä¢ internet_service ‚Üí ['internet_service_dsl', 'internet_service_fiber', 'internet_service_no']\n",
      "\n",
      "AUC with categorical features: 0.770\n",
      "Improvement: +0.101\n"
     ]
    }
   ],
   "source": [
    "# Before encoding - show the problem\n",
    "print(\"=== BEFORE ONE-HOT ENCODING ===\")\n",
    "numeric_cols = ['age', 'income', 'months_tenure']\n",
    "X_numeric_only = df[numeric_cols]\n",
    "y = df['churned']\n",
    "\n",
    "# Try training with numeric features only\n",
    "X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
    "    X_numeric_only, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
    "X_test_num_scaled = scaler.transform(X_test_num)\n",
    "\n",
    "model_numeric = LogisticRegression(random_state=42)\n",
    "model_numeric.fit(X_train_num_scaled, y_train)\n",
    "\n",
    "auc_numeric = roc_auc_score(y_test, model_numeric.predict_proba(X_test_num_scaled)[:, 1])\n",
    "print(f\"AUC with numeric features only: {auc_numeric:.3f}\")\n",
    "\n",
    "# One-hot encoding\n",
    "print(f\"\\n=== AFTER ONE-HOT ENCODING ===\")\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols)\n",
    "\n",
    "print(f\"Original features: {len(df.columns)}\")\n",
    "print(f\"After encoding: {len(df_encoded.columns)}\")\n",
    "print(f\"New categorical features created:\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    new_cols = [c for c in df_encoded.columns if c.startswith(col)]\n",
    "    print(f\"‚Ä¢ {col} ‚Üí {new_cols}\")\n",
    "\n",
    "# Train with all features\n",
    "X_encoded = df_encoded.drop('churned', axis=1)\n",
    "X_train_enc, X_test_enc, _, _ = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler_enc = StandardScaler()\n",
    "X_train_enc_scaled = scaler_enc.fit_transform(X_train_enc)\n",
    "X_test_enc_scaled = scaler_enc.transform(X_test_enc)\n",
    "\n",
    "model_encoded = LogisticRegression(random_state=42)\n",
    "model_encoded.fit(X_train_enc_scaled, y_train)\n",
    "\n",
    "auc_encoded = roc_auc_score(y_test, model_encoded.predict_proba(X_test_enc_scaled)[:, 1])\n",
    "print(f\"\\nAUC with categorical features: {auc_encoded:.3f}\")\n",
    "print(f\"Improvement: {auc_encoded - auc_numeric:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa35c89-ef47-4910-93ee-1256d525338a",
   "metadata": {},
   "source": [
    "### 5.3 Polynomial Features - Creating Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25f5dbb7-8e08-4eb2-9574-4c33fcb33c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POLYNOMIAL FEATURES FOR INTERACTIONS ===\n",
      "Original features:\n",
      "['age', 'income', 'months_tenure']\n",
      "\n",
      "After polynomial features (degree=2):\n",
      "Feature count: 3 ‚Üí 9\n",
      "New features created:\n",
      "‚Ä¢ age^2\n",
      "‚Ä¢ age income\n",
      "‚Ä¢ age months_tenure\n",
      "‚Ä¢ income^2\n",
      "‚Ä¢ income months_tenure\n",
      "‚Ä¢ months_tenure^2\n",
      "\n",
      "Performance comparison:\n",
      "‚Ä¢ Numeric only: 0.669\n",
      "‚Ä¢ With categoricals: 0.770\n",
      "‚Ä¢ With polynomials: 0.669\n",
      "\n",
      "Top 5 most important polynomial features:\n",
      "                feature  coefficient\n",
      "2         months_tenure       -0.464\n",
      "5     age months_tenure       -0.204\n",
      "7  income months_tenure       -0.184\n",
      "6              income^2       -0.144\n",
      "0                   age       -0.141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Start with simple numeric features for polynomial demo\n",
    "simple_features = df[['age', 'income', 'months_tenure']]\n",
    "\n",
    "print(\"=== POLYNOMIAL FEATURES FOR INTERACTIONS ===\")\n",
    "print(\"Original features:\")\n",
    "print(simple_features.columns.tolist())\n",
    "\n",
    "# Create polynomial features (degree 2 includes interactions)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "X_poly = poly.fit_transform(simple_features)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = poly.get_feature_names_out(simple_features.columns)\n",
    "print(f\"\\nAfter polynomial features (degree=2):\")\n",
    "print(f\"Feature count: {simple_features.shape[1]} ‚Üí {X_poly.shape[1]}\")\n",
    "print(\"New features created:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    if i >= 3:  # Skip original features\n",
    "        print(f\"‚Ä¢ {name}\")\n",
    "\n",
    "# Train model with polynomial features\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n",
    "    X_poly, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler_poly = StandardScaler()\n",
    "X_train_poly_scaled = scaler_poly.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler_poly.transform(X_test_poly)\n",
    "\n",
    "# Use regularization since we have more features now\n",
    "model_poly = LogisticRegression(C=0.1, random_state=42, max_iter=2000)\n",
    "model_poly.fit(X_train_poly_scaled, y_train_poly)\n",
    "\n",
    "auc_poly = roc_auc_score(y_test_poly, model_poly.predict_proba(X_test_poly_scaled)[:, 1])\n",
    "\n",
    "print(f\"\\nPerformance comparison:\")\n",
    "print(f\"‚Ä¢ Numeric only: {auc_numeric:.3f}\")\n",
    "print(f\"‚Ä¢ With categoricals: {auc_encoded:.3f}\")\n",
    "print(f\"‚Ä¢ With polynomials: {auc_poly:.3f}\")\n",
    "\n",
    "# Show most important polynomial features\n",
    "coef_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': model_poly.coef_[0],\n",
    "    'abs_coefficient': np.abs(model_poly.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 5 most important polynomial features:\")\n",
    "print(coef_importance.head()[['feature', 'coefficient']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce5996-042b-4827-be9c-286f6d4c326f",
   "metadata": {},
   "source": [
    "### 5.4 Feature Selection - Finding What Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d6ff688-6a81-4cbc-8225-a8271817d005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SELECTION METHODS ===\n",
      "Statistical selection (top 10 features):\n",
      "‚Ä¢ age\n",
      "‚Ä¢ income\n",
      "‚Ä¢ months_tenure\n",
      "‚Ä¢ contract_type_annual\n",
      "‚Ä¢ contract_type_monthly\n",
      "‚Ä¢ contract_type_two_year\n",
      "‚Ä¢ payment_method_bank_transfer\n",
      "‚Ä¢ payment_method_credit_card\n",
      "‚Ä¢ payment_method_electronic_check\n",
      "‚Ä¢ internet_service_no\n",
      "\n",
      "RFE selection (top 10 features):\n",
      "‚Ä¢ age\n",
      "‚Ä¢ income\n",
      "‚Ä¢ months_tenure\n",
      "‚Ä¢ contract_type_annual\n",
      "‚Ä¢ contract_type_monthly\n",
      "‚Ä¢ contract_type_two_year\n",
      "‚Ä¢ payment_method_bank_transfer\n",
      "‚Ä¢ payment_method_credit_card\n",
      "‚Ä¢ payment_method_electronic_check\n",
      "‚Ä¢ internet_service_dsl\n",
      "\n",
      "L1 regularization selection (6 features):\n",
      "‚Ä¢ age\n",
      "‚Ä¢ income\n",
      "‚Ä¢ months_tenure\n",
      "‚Ä¢ contract_type_monthly\n",
      "‚Ä¢ payment_method_electronic_check\n",
      "‚Ä¢ internet_service_dsl\n",
      "\n",
      "=== FEATURE SELECTION PERFORMANCE ===\n",
      "Method               Features   AUC     \n",
      "----------------------------------------\n",
      "All features         12         0.770\n",
      "Statistical          10         0.772\n",
      "RFE                  10         0.770\n",
      "L1 regularization    6          0.771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "print(\"=== FEATURE SELECTION METHODS ===\")\n",
    "\n",
    "# Method 1: Statistical selection (SelectKBest)\n",
    "# Select top 10 features based on ANOVA F-test\n",
    "selector_stats = SelectKBest(score_func=f_classif, k=10)\n",
    "X_selected_stats = selector_stats.fit_transform(X_train_enc_scaled, y_train)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features_stats = X_encoded.columns[selector_stats.get_support()]\n",
    "print(\"Statistical selection (top 10 features):\")\n",
    "for feature in selected_features_stats:\n",
    "    print(f\"‚Ä¢ {feature}\")\n",
    "\n",
    "# Method 2: Recursive Feature Elimination (RFE)\n",
    "# Use logistic regression to rank features\n",
    "estimator = LogisticRegression(C=1.0, random_state=42, max_iter=2000)\n",
    "selector_rfe = RFE(estimator, n_features_to_select=10, step=1)\n",
    "X_selected_rfe = selector_rfe.fit_transform(X_train_enc_scaled, y_train)\n",
    "\n",
    "selected_features_rfe = X_encoded.columns[selector_rfe.get_support()]\n",
    "print(f\"\\nRFE selection (top 10 features):\")\n",
    "for feature in selected_features_rfe:\n",
    "    print(f\"‚Ä¢ {feature}\")\n",
    "\n",
    "# Method 3: L1 regularization (automatic feature selection)\n",
    "model_l1 = LogisticRegression(penalty='l1', C=0.1, solver='liblinear', random_state=42)\n",
    "model_l1.fit(X_train_enc_scaled, y_train)\n",
    "\n",
    "# Features with non-zero coefficients\n",
    "l1_selected = X_encoded.columns[np.abs(model_l1.coef_[0]) > 0.001]\n",
    "print(f\"\\nL1 regularization selection ({len(l1_selected)} features):\")\n",
    "for feature in l1_selected:\n",
    "    print(f\"‚Ä¢ {feature}\")\n",
    "\n",
    "# Compare performance of different selection methods\n",
    "selection_results = {}\n",
    "\n",
    "# Original (all features)\n",
    "auc_all = roc_auc_score(y_test, model_encoded.predict_proba(X_test_enc_scaled)[:, 1])\n",
    "selection_results['All features'] = auc_all\n",
    "\n",
    "# Statistical selection\n",
    "model_stats = LogisticRegression(random_state=42)\n",
    "model_stats.fit(X_selected_stats, y_train)\n",
    "X_test_stats = selector_stats.transform(X_test_enc_scaled)\n",
    "auc_stats = roc_auc_score(y_test, model_stats.predict_proba(X_test_stats)[:, 1])\n",
    "selection_results['Statistical (10)'] = auc_stats\n",
    "\n",
    "# RFE selection\n",
    "model_rfe = LogisticRegression(random_state=42)\n",
    "model_rfe.fit(X_selected_rfe, y_train)\n",
    "X_test_rfe = selector_rfe.transform(X_test_enc_scaled)\n",
    "auc_rfe = roc_auc_score(y_test, model_rfe.predict_proba(X_test_rfe)[:, 1])\n",
    "selection_results['RFE (10)'] = auc_rfe\n",
    "\n",
    "# L1 selection\n",
    "auc_l1 = roc_auc_score(y_test, model_l1.predict_proba(X_test_enc_scaled)[:, 1])\n",
    "selection_results['L1 regularization'] = auc_l1\n",
    "\n",
    "print(f\"\\n=== FEATURE SELECTION PERFORMANCE ===\")\n",
    "print(f\"{'Method':<20} {'Features':<10} {'AUC':<8}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'All features':<20} {X_encoded.shape[1]:<10} {auc_all:.3f}\")\n",
    "print(f\"{'Statistical':<20} {10:<10} {auc_stats:.3f}\")\n",
    "print(f\"{'RFE':<20} {10:<10} {auc_rfe:.3f}\")\n",
    "print(f\"{'L1 regularization':<20} {len(l1_selected):<10} {auc_l1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c6c4d-5078-4b67-a2ff-1f5856ef7c28",
   "metadata": {},
   "source": [
    "### 5.5 Real-World Example - Complete Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "188311b7-d983-42fe-86fd-123df5571ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering pipeline:\n",
      "‚Ä¢ Numeric features: 3\n",
      "‚Ä¢ Categorical features: 3\n",
      "‚Ä¢ After one-hot encoding: 12 features\n",
      "‚Ä¢ Added income_per_age ratio\n",
      "‚Ä¢ Added customer lifecycle features\n",
      "‚Ä¢ Final feature count: 15\n",
      "\n",
      "=== FEATURE ENGINEERING IMPACT ===\n",
      "‚Ä¢ Baseline (numeric only): 0.669\n",
      "‚Ä¢ + Categorical encoding: 0.770\n",
      "‚Ä¢ + Complete engineering: 0.769\n",
      "‚Ä¢ Total improvement: +0.100\n",
      "\n",
      "Top 8 most important features:\n",
      "1. months_tenure: -0.727 ‚Üì\n",
      "2. contract_type_monthly: 0.399 ‚Üë\n",
      "3. age: -0.340 ‚Üì\n",
      "4. income: -0.261 ‚Üì\n",
      "5. contract_type_annual: -0.247 ‚Üì\n",
      "6. payment_method_electronic_check: 0.231 ‚Üë\n",
      "7. contract_type_two_year: -0.224 ‚Üì\n",
      "8. is_new_customer: 0.170 ‚Üë\n"
     ]
    }
   ],
   "source": [
    "# Create a complete feature engineering function\n",
    "def engineer_features(df, target_col):\n",
    "    \"\"\"Complete feature engineering pipeline\"\"\"\n",
    "    \n",
    "    # Separate target\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Identify feature types\n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"Feature engineering pipeline:\")\n",
    "    print(f\"‚Ä¢ Numeric features: {len(numeric_features)}\")\n",
    "    print(f\"‚Ä¢ Categorical features: {len(categorical_features)}\")\n",
    "    \n",
    "    # 1. One-hot encode categorical features\n",
    "    if categorical_features:\n",
    "        X_encoded = pd.get_dummies(X, columns=categorical_features, prefix=categorical_features)\n",
    "        print(f\"‚Ä¢ After one-hot encoding: {X_encoded.shape[1]} features\")\n",
    "    else:\n",
    "        X_encoded = X.copy()\n",
    "    \n",
    "    # 2. Create some domain-specific features\n",
    "    if 'age' in X_encoded.columns and 'income' in X_encoded.columns:\n",
    "        X_encoded['income_per_age'] = X_encoded['income'] / (X_encoded['age'] + 1)  # Avoid division by zero\n",
    "        print(f\"‚Ä¢ Added income_per_age ratio\")\n",
    "    \n",
    "    if 'months_tenure' in X_encoded.columns:\n",
    "        X_encoded['is_new_customer'] = (X_encoded['months_tenure'] < 12).astype(int)\n",
    "        X_encoded['is_loyal_customer'] = (X_encoded['months_tenure'] > 36).astype(int)\n",
    "        print(f\"‚Ä¢ Added customer lifecycle features\")\n",
    "    \n",
    "    print(f\"‚Ä¢ Final feature count: {X_encoded.shape[1]}\")\n",
    "    \n",
    "    return X_encoded, y\n",
    "\n",
    "# Apply complete feature engineering\n",
    "X_final, y_final = engineer_features(df.copy(), 'churned')\n",
    "\n",
    "# Train final model\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, random_state=42, stratify=y_final\n",
    ")\n",
    "\n",
    "scaler_final = StandardScaler()\n",
    "X_train_final_scaled = scaler_final.fit_transform(X_train_final)\n",
    "X_test_final_scaled = scaler_final.transform(X_test_final)\n",
    "\n",
    "model_final = LogisticRegression(C=0.1, random_state=42, max_iter=2000)\n",
    "model_final.fit(X_train_final_scaled, y_train_final)\n",
    "\n",
    "auc_final = roc_auc_score(y_test_final, model_final.predict_proba(X_test_final_scaled)[:, 1])\n",
    "\n",
    "print(f\"\\n=== FEATURE ENGINEERING IMPACT ===\")\n",
    "print(f\"‚Ä¢ Baseline (numeric only): {auc_numeric:.3f}\")\n",
    "print(f\"‚Ä¢ + Categorical encoding: {auc_encoded:.3f}\")\n",
    "print(f\"‚Ä¢ + Complete engineering: {auc_final:.3f}\")\n",
    "print(f\"‚Ä¢ Total improvement: {auc_final - auc_numeric:+.3f}\")\n",
    "\n",
    "# Show most important engineered features\n",
    "feature_importance_final = pd.DataFrame({\n",
    "    'feature': X_final.columns,\n",
    "    'coefficient': model_final.coef_[0],\n",
    "    'abs_coefficient': np.abs(model_final.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 8 most important features:\")\n",
    "for i, (_, row) in enumerate(feature_importance_final.head(8).iterrows()):\n",
    "    direction = \"‚Üë\" if row['coefficient'] > 0 else \"‚Üì\"\n",
    "    print(f\"{i+1}. {row['feature']}: {row['coefficient']:.3f} {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181abf9f-ae1f-465d-b35e-54def155517e",
   "metadata": {},
   "source": [
    "## Section 6 - Production Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b256e-7ef2-4954-a9c0-22c40bf72eab",
   "metadata": {},
   "source": [
    "### 6.1 sklearn Pipeline - Preprocessing + Model in One Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36cf93b3-bc65-4384-8a43-e0934bae27a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRODUCTION PIPELINE SETUP ===\n",
      "Dataset shape: (2000, 7)\n",
      "Numeric features: ['age', 'income', 'months_tenure']\n",
      "Categorical features: ['contract_type', 'payment_method', 'internet_service']\n",
      "\n",
      "Pipeline steps:\n",
      "1. preprocessor: ColumnTransformer\n",
      "2. classifier: LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "# Create production-ready pipeline using previous churn dataset\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Recreate our customer dataset (same as Section 5)\n",
    "np.random.seed(42)\n",
    "n_customers = 2000\n",
    "\n",
    "data = {\n",
    "    'age': np.random.normal(40, 15, n_customers).astype(int),\n",
    "    'income': np.random.normal(50000, 20000, n_customers),\n",
    "    'months_tenure': np.random.uniform(1, 60, n_customers),\n",
    "    'contract_type': np.random.choice(['monthly', 'annual', 'two_year'], n_customers, p=[0.5, 0.3, 0.2]),\n",
    "    'payment_method': np.random.choice(['credit_card', 'bank_transfer', 'electronic_check'], n_customers, p=[0.4, 0.3, 0.3]),\n",
    "    'internet_service': np.random.choice(['dsl', 'fiber', 'no'], n_customers, p=[0.4, 0.4, 0.2])\n",
    "}\n",
    "\n",
    "# Create churn target\n",
    "churn_probability = (\n",
    "    -0.02 * data['age'] +\n",
    "    -0.00001 * data['income'] +\n",
    "    -0.05 * data['months_tenure'] +\n",
    "    (np.array(data['contract_type']) == 'monthly') * 1.5 +\n",
    "    (np.array(data['payment_method']) == 'electronic_check') * 0.8 +\n",
    "    2.0\n",
    ")\n",
    "churn_prob = 1 / (1 + np.exp(-churn_probability))\n",
    "data['churned'] = np.random.binomial(1, churn_prob)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"=== PRODUCTION PIPELINE SETUP ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Define feature types for pipeline\n",
    "numeric_features = ['age', 'income', 'months_tenure']\n",
    "categorical_features = ['contract_type', 'payment_method', 'internet_service']\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing steps\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "    ('scaler', StandardScaler())                     # Scale features\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encode\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Create complete pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(C=0.1, random_state=42, max_iter=2000))\n",
    "])\n",
    "\n",
    "print(f\"\\nPipeline steps:\")\n",
    "for i, (name, step) in enumerate(pipeline.steps):\n",
    "    print(f\"{i+1}. {name}: {type(step).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a233ae1-bbaa-48a4-ac20-71f8a71f7e3e",
   "metadata": {},
   "source": [
    "### 6.2 Training and Evaluating the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85a20771-57c8-437d-9edd-ffa979b3de38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING PRODUCTION PIPELINE ===\n",
      "Pipeline Performance:\n",
      "‚Ä¢ Accuracy: 0.718\n",
      "‚Ä¢ ROC-AUC: 0.770\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Retained       0.69      0.70      0.70       186\n",
      "     Churned       0.74      0.73      0.73       214\n",
      "\n",
      "    accuracy                           0.72       400\n",
      "   macro avg       0.72      0.72      0.72       400\n",
      "weighted avg       0.72      0.72      0.72       400\n",
      "\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "‚Ä¢ months_tenure: -0.857 (decreases churn probability)\n",
      "‚Ä¢ contract_type_monthly: 0.823 (increases churn probability)\n",
      "‚Ä¢ payment_method_electronic_check: 0.437 (increases churn probability)\n",
      "‚Ä¢ contract_type_two_year: -0.430 (decreases churn probability)\n",
      "‚Ä¢ contract_type_annual: -0.413 (decreases churn probability)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop('churned', axis=1)\n",
    "y = df['churned']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"=== TRAINING PRODUCTION PIPELINE ===\")\n",
    "\n",
    "# Train pipeline (handles all preprocessing automatically)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(f\"Pipeline Performance:\")\n",
    "print(f\"‚Ä¢ Accuracy: {accuracy:.3f}\")\n",
    "print(f\"‚Ä¢ ROC-AUC: {auc:.3f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Retained', 'Churned']))\n",
    "\n",
    "# Show what pipeline learned\n",
    "feature_names = (numeric_features + \n",
    "                list(pipeline.named_steps['preprocessor']\n",
    "                    .named_transformers_['cat']\n",
    "                    .named_steps['onehot']\n",
    "                    .get_feature_names_out(categorical_features)))\n",
    "\n",
    "coefficients = pipeline.named_steps['classifier'].coef_[0]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "for _, row in feature_importance.head().iterrows():\n",
    "    direction = \"increases\" if row['coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"‚Ä¢ {row['feature']}: {row['coefficient']:.3f} ({direction} churn probability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872760e-b0b7-48e8-954a-921d3f496517",
   "metadata": {},
   "source": [
    "### 6.3 Handling New Data - Edge Cases and Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5d08f24-5ab2-4020-9274-3fa41f1f981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING PIPELINE ROBUSTNESS ===\n",
      "Edge cases to test:\n",
      "    age    income  months_tenure     contract_type      payment_method  \\\n",
      "0  25.0   30000.0            1.0           monthly         credit_card   \n",
      "1   NaN   80000.0           24.0            annual  new_payment_method   \n",
      "2  70.0       NaN            NaN  unknown_contract       bank_transfer   \n",
      "3  18.0  200000.0            0.5          two_year    electronic_check   \n",
      "4  45.0   45000.0           48.0           monthly                 NaN   \n",
      "\n",
      "  internet_service  \n",
      "0            fiber  \n",
      "1              dsl  \n",
      "2        satellite  \n",
      "3               no  \n",
      "4            fiber  \n",
      "\n",
      "‚úÖ Pipeline successfully handled all edge cases!\n",
      "\n",
      "Predictions:\n",
      "Case   Prediction   Churn Prob  Confidence\n",
      "---------------------------------------------\n",
      "1      Will Churn   0.929       0.929\n",
      "2      Will Stay    0.385       0.615\n",
      "3      Will Stay    0.284       0.716\n",
      "4      Will Churn   0.526       0.526\n",
      "5      Will Stay    0.453       0.547\n",
      "\n",
      "üîç HOW PIPELINE HANDLES EDGE CASES:\n",
      "‚Ä¢ Missing numeric values ‚Üí Filled with median\n",
      "‚Ä¢ Missing categorical values ‚Üí Filled with 'missing' category\n",
      "‚Ä¢ Unknown categories ‚Üí Ignored (all zeros in one-hot encoding)\n",
      "‚Ä¢ Extreme values ‚Üí Scaled with same scaler from training\n"
     ]
    }
   ],
   "source": [
    "# Create challenging test cases that could break a naive model\n",
    "edge_cases = pd.DataFrame({\n",
    "    'age': [25, np.nan, 70, 18, 45],  # Missing value, extreme values\n",
    "    'income': [30000, 80000, np.nan, 200000, 45000],  # Missing income, very high income\n",
    "    'months_tenure': [1, 24, np.nan, 0.5, 48],  # Missing tenure, very short tenure\n",
    "    'contract_type': ['monthly', 'annual', 'unknown_contract', 'two_year', 'monthly'],  # Unknown category\n",
    "    'payment_method': ['credit_card', 'new_payment_method', 'bank_transfer', 'electronic_check', np.nan],  # Unknown method, missing\n",
    "    'internet_service': ['fiber', 'dsl', 'satellite', 'no', 'fiber']  # Unknown service type\n",
    "})\n",
    "\n",
    "print(\"=== TESTING PIPELINE ROBUSTNESS ===\")\n",
    "print(\"Edge cases to test:\")\n",
    "print(edge_cases)\n",
    "\n",
    "# Pipeline handles all edge cases automatically!\n",
    "try:\n",
    "    predictions = pipeline.predict(edge_cases)\n",
    "    probabilities = pipeline.predict_proba(edge_cases)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline successfully handled all edge cases!\")\n",
    "    \n",
    "    print(f\"\\nPredictions:\")\n",
    "    print(f\"{'Case':<6} {'Prediction':<12} {'Churn Prob':<11} {'Confidence'}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for i in range(len(edge_cases)):\n",
    "        pred_label = 'Will Churn' if predictions[i] == 1 else 'Will Stay'\n",
    "        churn_prob = probabilities[i, 1]\n",
    "        confidence = max(probabilities[i])\n",
    "        \n",
    "        print(f\"{i+1:<6} {pred_label:<12} {churn_prob:<11.3f} {confidence:.3f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pipeline failed: {e}\")\n",
    "\n",
    "# Show how pipeline handles missing/unknown values\n",
    "print(f\"\\nüîç HOW PIPELINE HANDLES EDGE CASES:\")\n",
    "print(f\"‚Ä¢ Missing numeric values ‚Üí Filled with median\")\n",
    "print(f\"‚Ä¢ Missing categorical values ‚Üí Filled with 'missing' category\")\n",
    "print(f\"‚Ä¢ Unknown categories ‚Üí Ignored (all zeros in one-hot encoding)\")\n",
    "print(f\"‚Ä¢ Extreme values ‚Üí Scaled with same scaler from training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df37c75-d9a9-455b-b479-12215dff732c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
